{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a text string to examine with NEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify query string\n",
    "payload = 'The World Health Organization on Sunday reported the largest single-day increase in coronavirus cases by its count, at more than 183,000 new cases in the latest 24 hours. The UN health agency said Brazil led the way with 54,771 cases tallied and the U.S. next at 36,617. Over 15,400 came in in India.'\n",
    "payload = 'is strongly affected by large ground-water withdrawals at or near Tupelo, Aberdeen, and West Point.'\n",
    "# payload = 'Overall design: Teliospores of pathogenic races T-1, T-5 and T-16 of T. caries provided by a collection in Aberdeen, ID, USA'\n",
    "payload = 'The results provide evidence of substantial population structure in C. posadasii and demonstrate presence of distinct geographic clades in Central and Southern Arizona as well as dispersed populations in Texas, Mexico and South and Central America'\n",
    "payload = 'Most frequent numerical abnormalities in B-NHL were gains of chromosomes 3 and 18, although gains of chromosome 3 were less prominent in FL.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import credentials file\n",
    "import yaml\n",
    "with open(\"config.yml\", 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general way to extract values for a given key. Returns an array. Used to parse Nemo response and extract wikipedia id\n",
    "# from https://hackersandslackers.com/extract-data-from-complex-json-python/\n",
    "\n",
    "def extract_values(obj, key):\n",
    "    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n",
    "    arr = []\n",
    "\n",
    "    def extract(obj, arr, key):\n",
    "        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                if isinstance(v, (dict, list)):\n",
    "                    extract(v, arr, key)\n",
    "                elif k == key:\n",
    "                    arr.append(v)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extract(item, arr, key)\n",
    "        return arr\n",
    "\n",
    "    results = extract(obj, arr, key)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting wikipedia ID\n",
    "# see he API at https://www.mediawiki.org/wiki/API:Query#Example_5:_Batchcomplete\n",
    "# also, https://stackoverflow.com/questions/37024807/how-to-get-wikidata-id-for-an-wikipedia-article-by-api\n",
    "\n",
    "def get_WPID (name):\n",
    "    import json\n",
    "    url = 'https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&ppprop=wikibase_item&redirects=1&format=json&titles=' +name\n",
    "    r=requests.get(url).json()\n",
    "    return extract_values(r,'wikibase_item')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a request to NEMO, and get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a service request\n",
    "import requests\n",
    "\n",
    "# payloadutf = payload.encode('utf-8')\n",
    "\n",
    "url = \"https://nemoservice.azurewebsites.net/nemo?appid=\" + cfg['api_creds']['nmo1']\n",
    "newHeaders = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "response = requests.post(url,\n",
    "                         data='\"{' + payload + '}\"',\n",
    "                         headers=newHeaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most frequent numerical abnormalities in <e ref=\"B-NHL\" type=\"U\" name=\"B-NHL\" form=\"B-NHL\" wp=\"n\">B-NHL</e> were gains of chromosomes 3 and 18, although gains of <c ref=\"chromosome\" type=\"U\" name=\"chromosome\" form=\"chromosome\" wp=\"n\">chromosome</c> 3 were less prominent in <e ref=\"Florida\" type=\"G\" name=\"Florida\" form=\"FL.\" wp=\"y\">FL.</e>'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the results as string (remove json braces)\n",
    "a = response.content.decode()\n",
    "resp_full = a[a.find('{')+1 : a.find('}')]\n",
    "resp_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the response and load all found elements into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with entities, remove duplicates, then add wikipedia/wikidata concept IDs\n",
    "import pandas as pd\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "df = pd.DataFrame(columns=[\"Type\",\"Ref\",\"EntityType\",\"Name\",\"Form\",\"WP\",\"Value\",\"Alt\",\"WP_ID\"])\n",
    "\n",
    "# note that the last column is to be populated later, via Wikipedia API\n",
    "# all previous columns are from Nemo: based on \"e\" (entity) and \"d\" (data) elements. \"c\" (concept) to be explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get starting and ending positions of xml fragments in the Nemo output\n",
    "pattern_start = \"<(e|d|c)\\s\"\n",
    "iter = re.finditer(pattern_start,resp_full)\n",
    "indices1 = [m.start(0) for m in iter]\n",
    "pattern_end = \"</(e|d|c)>\"\n",
    "iter = re.finditer(pattern_end,resp_full)\n",
    "indices2 = [m.start(0) for m in iter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over xml fragments returned by Nemo, extracting attributes from each and adding to dataframe\n",
    "for i, entity in enumerate(indices1):\n",
    "    a = resp_full[indices1[i] : indices2[i]+4]\n",
    "\n",
    "    root = ET.fromstring(a)\n",
    "    tag = root.tag\n",
    "    attributes = root.attrib\n",
    "\n",
    "    df = df.append({\"Type\":root.tag, \n",
    "                \"Ref\":attributes.get('ref'),\n",
    "                \"EntityType\":attributes.get('type'),\n",
    "                \"Name\":attributes.get('name'),\n",
    "                \"Form\":attributes.get('form'),\n",
    "                \"WP\":attributes.get('wp'),\n",
    "                \"Value\":attributes.get('value'),\n",
    "                \"Alt\":attributes.get('alt')},\n",
    "               ignore_index=True)        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E stands for entity; \n",
    "\n",
    "the attribute ref gives you the title of the corresponding Wikipedia page when the attribute wp has the value “y”; \n",
    "\n",
    "the attribute type gives you the type of entity for known entities; the types of interest for you are G, which is geo-political entity, L – geographic form/location (such as a mountain), and F, which is facility (such as an airport).\n",
    "\n",
    "D stands for datafield, which comprises dates, NUMEX, email addresses and URLs, tracking numbers, and so on.\n",
    "\n",
    "C stands for concept; these appear in Wikipedia and are deemed as relevant for the input text, but they do not get disambiguated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate records from the df\n",
    "df = df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each found entity, add wikidata unique identifiers to the dataframe\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['WP']=='y'):\n",
    "        row['WP_ID'] = get_WPID(row['Name'])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Ref</th>\n",
       "      <th>EntityType</th>\n",
       "      <th>Name</th>\n",
       "      <th>Form</th>\n",
       "      <th>WP</th>\n",
       "      <th>Value</th>\n",
       "      <th>Alt</th>\n",
       "      <th>WP_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>B-NHL</td>\n",
       "      <td>U</td>\n",
       "      <td>B-NHL</td>\n",
       "      <td>B-NHL</td>\n",
       "      <td>n</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "      <td>chromosome</td>\n",
       "      <td>U</td>\n",
       "      <td>chromosome</td>\n",
       "      <td>chromosome</td>\n",
       "      <td>n</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>Florida</td>\n",
       "      <td>G</td>\n",
       "      <td>Florida</td>\n",
       "      <td>FL.</td>\n",
       "      <td>y</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type         Ref EntityType        Name        Form WP Value   Alt WP_ID\n",
       "0    e       B-NHL          U       B-NHL       B-NHL  n  None  None   NaN\n",
       "1    c  chromosome          U  chromosome  chromosome  n  None  None   NaN\n",
       "2    e     Florida          G     Florida         FL.  y  None  None  Q812"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
