{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create local Neo4j Knowledge Graph (for testing)\n",
    "This notebook is provided to test creating a knowledge graph using a locally installed version of Neo4j.\n",
    "\n",
    "**Note, make sure that the Neo4j Browser has been shutdown before running this notebook.**\n",
    "\n",
    "The path `NEO4J_HOME` must be set to the Neo4j installation directory path.\n",
    "\n",
    "This notebook creates a local version of the knowledge graph by ingesting the .csv files in the Neo4j import directory `NEO4J_HOME/import`.\n",
    "\n",
    "Note: This notebook works only on Linux and Mac OS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0\n"
     ]
    }
   ],
   "source": [
    "NEO4J_HOME = Path(os.getenv('NEO4J_HOME'))\n",
    "print(NEO4J_HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Neo4j database\n",
    "\n",
    "**Note, make sure that the Neo4j Browser has been shutdown before running this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories in use:\n",
      "  home:         /Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0\n",
      "  config:       /Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0/conf\n",
      "  logs:         /Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0/logs\n",
      "  plugins:      /Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0/plugins\n",
      "  import:       /Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0/import\n",
      "  data:         /Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0/data\n",
      "  certificates: /Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0/certificates\n",
      "  run:          /Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0/run\n",
      "Starting Neo4j.\n",
      "Started neo4j (pid 4808). It is available at http://localhost:7474/\n",
      "There may be a short delay until the server is ready.\n",
      "See /Users/peter/Library/Application Support/Neo4j Desktop/Application/neo4jDatabases/database-328d8379-6ab4-4cc1-a397-2de37909d2e4/installation-4.1.0/logs/neo4j.log for current status.\n",
      "['Neo4j is running at pid 4808']\n"
     ]
    }
   ],
   "source": [
    "status = !\"$NEO4J_HOME\"/bin/neo4j status\n",
    "status = str(status)\n",
    "while not 'Neo4j is running' in status:\n",
    "    !\"$NEO4J_HOME\"/bin/neo4j start\n",
    "    time.sleep(30)\n",
    "    status = !\"$NEO4J_HOME\"/bin/neo4j status\n",
    "    status = str(status)\n",
    "    print(status)\n",
    "\n",
    "# wait until neo4j is ready to receive requests\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Cypher commands to create Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: bolt://localhost:7687\n",
      "Username: neo4j\n",
      "Password: neo4jbinder\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00a-Init.cypher:\n",
      " \n",
      "// delete all nodes\n",
      "//MATCH (n) DETACH DELETE n; # deleting all nodes at once leads to out of memory error\n",
      "// Delete in small batches\n",
      "call apoc.periodic.iterate(\"MATCH (n) return n\", \"DETACH DELETE n\", {batchSize:1000}) yield batches, total \n",
      "RETURN batches, total;\n",
      "\n",
      "// delete all constraints and indices\n",
      "CALL db.index.fulltext.drop('locations');\n",
      "CALL db.index.fulltext.drop('bioentities');\n",
      "CALL db.index.fulltext.drop('geoids');\n",
      "CALL apoc.schema.assert({},{}, true);\n",
      "                                                                                            \n",
      "// create constraints and indices\n",
      "CREATE CONSTRAINT location ON (n:Location) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX location_n FOR (n:Location) ON (n.name);\n",
      "CREATE INDEX location_l FOR (n:Location) ON (n.location);\n",
      "CREATE CONSTRAINT world ON (n:World) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT unregion ON (n:UNRegion) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT unsubregion ON (n:UNSubRegion) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT unintermediateregion ON (n:UNIntermediateRegion) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT country ON (n:Country) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX country_n FOR (n:Country) ON (n.name);\n",
      "CREATE INDEX country_g FOR (n:Country) ON (n.geonameId);\n",
      "CREATE INDEX country_l FOR (n:Country) ON (n.location);\n",
      "CREATE CONSTRAINT admin1 ON (n:Admin1) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX admin1_n FOR (n:Admin1) ON (n.name);\n",
      "CREATE INDEX admin1_f FOR (n:Admin1) ON (n.fips);\n",
      "CREATE INDEX admin1_p FOR (n:Admin1) ON (n.parentId);\n",
      "CREATE INDEX admin1_g FOR (n:Admin1) ON (n.geonameId);\n",
      "CREATE INDEX admin1_l FOR (n:Admin1) ON (n.location);\n",
      "CREATE CONSTRAINT usregion ON (n:USRegion) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT usdivision ON (n:USDivision) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT admin2 ON (n:Admin2) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX admin2_f FOR (n:Admin2) ON (n.fips);\n",
      "CREATE INDEX admin2_s FOR (n:Admin2) ON (n.stateFips);\n",
      "CREATE INDEX admin2_g FOR (n:Admin2) ON (n.geonameId);\n",
      "CREATE INDEX admin2_n FOR (n:Admin2) ON (n.name);\n",
      "CREATE INDEX admin2_l FOR (n:Admin2) ON (n.location);\n",
      "CREATE CONSTRAINT city ON (n:City) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX city_n FOR (n:City) ON (n.name);\n",
      "CREATE INDEX city_l FOR (n:City) ON (n.location);\n",
      "CREATE INDEX city_g FOR (n:City) ON (n.geonameId);\n",
      "CREATE CONSTRAINT postalcode ON (n:PostalCode) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX postalcode_n FOR (n:PostalCode) ON (n.name);\n",
      "CREATE INDEX postalcode_p FOR (n:PostalCode) ON (n.placeName);\n",
      "CREATE INDEX postalcode_l FOR (n:PostalCode) ON (n.location);\n",
      "CREATE CONSTRAINT tract ON (n:Tract) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT cruiseship ON (n:CruiseShip) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX cruiseship_o FOR (n:CruiseShip) ON (n.origLocation);\n",
      "                                   \n",
      "CREATE CONSTRAINT organism ON (n:Organism) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT outbreak ON (n:Outbreak) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT publication ON (n:Publication) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT strain ON (n:Strain) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX strain_n FOR (n:Strain) ON (n.name);\n",
      "CREATE INDEX strain_o FOR (n:Strain) ON (n.origLocation);\n",
      "CREATE CONSTRAINT variant ON (n:Variant) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT gene ON (n:Gene) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX gene_a FOR (n:Gene) ON (n.genomeAccession);\n",
      "CREATE INDEX gene_s FOR (n:Gene) ON (n.start);\n",
      "CREATE INDEX gene_e FOR (n:Gene) ON (n.end);\n",
      "CREATE CONSTRAINT protein ON (n:Protein) ASSERT n.id IS UNIQUE;\n",
      "CREATE CONSTRAINT proteinname ON (n:ProteinName) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX proteinname_n FOR (n:ProteinName) ON (n.name);\n",
      "CREATE INDEX proteinname_a FOR (n:ProteinName) ON (n.accession);\n",
      "CREATE CONSTRAINT cases ON (n:Cases) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX cases_d FOR (n:Cases) ON (n.date);\n",
      "CREATE INDEX cases_s FOR (n:Cases) ON (n.source);\n",
      "CREATE INDEX cases_o FOR (n:Cases) ON (n.origLocation);\n",
      "CREATE CONSTRAINT chain ON (n:Chain) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX chain_n FOR (n:Chain) ON (n.name);\n",
      "CREATE CONSTRAINT structure ON (n:Structure) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX structure_n FOR (n:Structure) ON (n.name);\n",
      "\n",
      "CREATE CONSTRAINT socialcharacteristics ON (n:SocialCharacteristics) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX socialcharacteristics_c FOR (n:SocialCharacteristics) ON (n.countyFips);\n",
      "CREATE INDEX socialcharacteristics_s FOR (n:SocialCharacteristics) ON (n.stateFips);\n",
      "CREATE INDEX socialcharacteristics_p FOR (n:SocialCharacteristics) ON (n.postalCode);\n",
      "CREATE INDEX socialcharacteristics_t FOR (n:SocialCharacteristics) ON (n.tract);\n",
      "CREATE CONSTRAINT education ON (n:Education) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX education_c FOR (n:Education) ON (n.countyFips);\n",
      "CREATE INDEX education_s FOR (n:Education) ON (n.stateFips);\n",
      "CREATE INDEX education_p FOR (n:Education) ON (n.postalCode);\n",
      "CREATE INDEX education_t FOR (n:Education) ON (n.tract);\n",
      "CREATE CONSTRAINT computers ON (n:Computers) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX computers_c FOR (n:Computers) ON (n.countyFips);\n",
      "CREATE INDEX computers_s FOR (n:Computers) ON (n.stateFips);\n",
      "CREATE INDEX computers_p FOR (n:Computers) ON (n.postalCode);\n",
      "CREATE INDEX computers_t FOR (n:Computers) ON (n.tract);\n",
      "CREATE CONSTRAINT economics ON (n:Economics) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX economics_c FOR (n:Economics) ON (n.countyFips);\n",
      "CREATE INDEX economics_s FOR (n:Economics) ON (n.stateFips);\n",
      "CREATE INDEX economics_p FOR (n:Economics) ON (n.postalCode);\n",
      "CREATE INDEX economics_t FOR (n:Economics) ON (n.tract);\n",
      "CREATE CONSTRAINT employment ON (n:Employment) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX employment_c FOR (n:Employment) ON (n.countyFips);\n",
      "CREATE INDEX employment_s FOR (n:Employment) ON (n.stateFips);\n",
      "CREATE INDEX employment_p FOR (n:Employment) ON (n.postalCode);\n",
      "CREATE INDEX employment_t FOR (n:Employment) ON (n.tract);\n",
      "CREATE CONSTRAINT income ON (n:Income) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX income_c FOR (n:Income) ON (n.countyFips);\n",
      "CREATE INDEX income_s FOR (n:Income) ON (n.stateFips);\n",
      "CREATE INDEX income_p FOR (n:Income) ON (n.postalCode);\n",
      "CREATE INDEX income_t FOR (n:Income) ON (n.tract);\n",
      "CREATE CONSTRAINT healthinsurance ON (n:HealthInsurance) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX healthinsurance_c FOR (n:HealthInsurance) ON (n.countyFips);\n",
      "CREATE INDEX healthinsurance_s FOR (n:HealthInsurance) ON (n.stateFips);\n",
      "CREATE INDEX healthinsurance_p FOR (n:HealthInsurance) ON (n.postalCode);\n",
      "CREATE INDEX healthinsurance_t FOR (n:HealthInsurance) ON (n.tract);\n",
      "CREATE CONSTRAINT commuting ON (n:Commuting) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX commuting_c FOR (n:Commuting) ON (n.countyFips);\n",
      "CREATE INDEX commuting_s FOR (n:Commuting) ON (n.stateFips);\n",
      "CREATE INDEX commuting_p FOR (n:Commuting) ON (n.postalCode);\n",
      "CREATE INDEX commuting_t FOR (n:Commuting) ON (n.tract);\n",
      "CREATE CONSTRAINT occupation ON (n:Occupation) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX occupation_c FOR (n:Occupation) ON (n.countyFips);\n",
      "CREATE INDEX occupation_s FOR (n:Occupation) ON (n.stateFips);\n",
      "CREATE INDEX occupation_p FOR (n:Occupation) ON (n.postalCode);\n",
      "CREATE INDEX occupation_t FOR (n:Occupation) ON (n.tract);\n",
      "CREATE CONSTRAINT poverty ON (n:Poverty) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX poverty_c FOR (n:Poverty) ON (n.countyFips);\n",
      "CREATE INDEX poverty_s FOR (n:Poverty) ON (n.stateFips);\n",
      "CREATE INDEX poverty_p FOR (n:Poverty) ON (n.postalCode);\n",
      "CREATE INDEX poverty_t FOR (n:Poverty) ON (n.tract);\n",
      "CREATE CONSTRAINT housing ON (n:Housing) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX housing_c FOR (n:Housing) ON (n.countyFips);\n",
      "CREATE INDEX housing_s FOR (n:Housing) ON (n.stateFips);\n",
      "CREATE INDEX housing_p FOR (n:Housing) ON (n.postalCode);\n",
      "CREATE INDEX housing_t FOR (n:Housing) ON (n.tract);\n",
      "CREATE CONSTRAINT demographics ON (n:Demographics) ASSERT n.id IS UNIQUE;\n",
      "CREATE INDEX demographics_c FOR (n:Demographics) ON (n.countyFips);\n",
      "CREATE INDEX demographics_s FOR (n:Demographics) ON (n.stateFips);\n",
      "CREATE INDEX demographics_p FOR (n:Demographics) ON (n.postalCode);\n",
      "CREATE INDEX demographics_t FOR (n:Demographics) ON (n.tract);\n",
      "\n",
      "\n",
      "// create full text search indices\n",
      "CALL db.index.fulltext.createNodeIndex('locations',['World', 'UNRegion', 'UNSubRegion', 'UNIntermediateRegion', 'Country', 'Admin1', 'Admin2', 'USRegion', 'USDivision', 'City', 'CruiseShip', 'PostalCode','Tract'],['name', 'placeName', 'iso', 'iso3', 'fips', 'geonameId', 'code', 'origLocation']);\n",
      "CALL db.index.fulltext.createNodeIndex('bioentities',['ProteinName', 'Protein', 'Gene', 'Strain', 'Variant', 'Organism', 'Outbreak','Chain','Structure'],['name', 'scientificName', 'taxonomyId', 'accession', 'proId', 'genomeAccession', 'geneVariant', 'proteinVariant', 'variantType', 'variantConsequence']);\n",
      "CALL db.index.fulltext.createNodeIndex('geoids',['UNRegion', 'UNSubRegion', 'UNIntermediateRegion', 'Country', 'Admin1', 'Admin2', 'USRegion', 'USDivision', 'City', 'PostalCode','Tract'],['id','iso', 'iso3', 'fips', 'geonameId','code','name'], {analyzer: 'keyword'});         \n",
      "\n",
      "\n",
      "\n",
      "// list constraints and indices\n",
      "CALL db.constraints();\n",
      "CALL db.indexes();batches, total\n",
      "3825, 3824487\n",
      "label, key, keys, unique, action\n",
      "\"Location\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"Location\", \"location\", [\"location\"], FALSE, \"DROPPED\"\n",
      "\"Country\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"Country\", \"geonameId\", [\"geonameId\"], FALSE, \"DROPPED\"\n",
      "\"Country\", \"location\", [\"location\"], FALSE, \"DROPPED\"\n",
      "\"Admin1\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"Admin1\", \"fips\", [\"fips\"], FALSE, \"DROPPED\"\n",
      "\"Admin1\", \"parentId\", [\"parentId\"], FALSE, \"DROPPED\"\n",
      "\"Admin1\", \"geonameId\", [\"geonameId\"], FALSE, \"DROPPED\"\n",
      "\"Admin1\", \"location\", [\"location\"], FALSE, \"DROPPED\"\n",
      "\"Admin2\", \"fips\", [\"fips\"], FALSE, \"DROPPED\"\n",
      "\"Admin2\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Admin2\", \"geonameId\", [\"geonameId\"], FALSE, \"DROPPED\"\n",
      "\"Admin2\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"Admin2\", \"location\", [\"location\"], FALSE, \"DROPPED\"\n",
      "\"City\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"City\", \"location\", [\"location\"], FALSE, \"DROPPED\"\n",
      "\"City\", \"geonameId\", [\"geonameId\"], FALSE, \"DROPPED\"\n",
      "\"PostalCode\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"PostalCode\", \"placeName\", [\"placeName\"], FALSE, \"DROPPED\"\n",
      "\"PostalCode\", \"location\", [\"location\"], FALSE, \"DROPPED\"\n",
      "\"CruiseShip\", \"origLocation\", [\"origLocation\"], FALSE, \"DROPPED\"\n",
      "\"Strain\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"Strain\", \"origLocation\", [\"origLocation\"], FALSE, \"DROPPED\"\n",
      "\"Gene\", \"genomeAccession\", [\"genomeAccession\"], FALSE, \"DROPPED\"\n",
      "\"Gene\", \"start\", [\"start\"], FALSE, \"DROPPED\"\n",
      "\"Gene\", \"end\", [\"end\"], FALSE, \"DROPPED\"\n",
      "\"ProteinName\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"ProteinName\", \"accession\", [\"accession\"], FALSE, \"DROPPED\"\n",
      "\"Cases\", \"date\", [\"date\"], FALSE, \"DROPPED\"\n",
      "\"Cases\", \"source\", [\"source\"], FALSE, \"DROPPED\"\n",
      "\"Cases\", \"origLocation\", [\"origLocation\"], FALSE, \"DROPPED\"\n",
      "\"Chain\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"Structure\", \"name\", [\"name\"], FALSE, \"DROPPED\"\n",
      "\"SocialCharacteristics\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"SocialCharacteristics\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"SocialCharacteristics\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"SocialCharacteristics\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Education\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Education\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Education\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Education\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Computers\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Computers\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Computers\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Computers\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Economics\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Economics\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Economics\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Economics\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Employment\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Employment\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Employment\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Employment\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Income\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Income\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Income\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Income\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"HealthInsurance\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"HealthInsurance\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"HealthInsurance\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"HealthInsurance\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Commuting\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Commuting\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Commuting\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Commuting\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Occupation\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Occupation\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Occupation\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Occupation\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Poverty\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Poverty\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Poverty\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Poverty\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Housing\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Housing\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Housing\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Housing\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"Demographics\", \"countyFips\", [\"countyFips\"], FALSE, \"DROPPED\"\n",
      "\"Demographics\", \"stateFips\", [\"stateFips\"], FALSE, \"DROPPED\"\n",
      "\"Demographics\", \"postalCode\", [\"postalCode\"], FALSE, \"DROPPED\"\n",
      "\"Demographics\", \"tract\", [\"tract\"], FALSE, \"DROPPED\"\n",
      "\"World\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Admin2\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Strain\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Structure\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Commuting\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"USRegion\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Outbreak\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Cases\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Income\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Country\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"CruiseShip\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Protein\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Economics\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Demographics\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"UNSubRegion\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"PostalCode\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Variant\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Education\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Poverty\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"SocialCharacteristics\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Occupation\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Location\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"USDivision\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Publication\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Chain\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"HealthInsurance\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Admin1\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Organism\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"ProteinName\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Employment\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"UNIntermediateRegion\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Tract\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Gene\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Computers\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"Housing\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"UNRegion\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "\"City\", \"id\", [\"id\"], TRUE, \"DROPPED\"\n",
      "name, description, details\n",
      "\"admin1\", \"CONSTRAINT ON ( admin1:Admin1 ) ASSERT (admin1.id) IS UNIQUE\", \"Constraint( id=178, name='admin1', type='UNIQUENESS', schema=(:Admin1 {id}), ownedIndex=177 )\"\n",
      "\"admin2\", \"CONSTRAINT ON ( admin2:Admin2 ) ASSERT (admin2.id) IS UNIQUE\", \"Constraint( id=189, name='admin2', type='UNIQUENESS', schema=(:Admin2 {id}), ownedIndex=188 )\"\n",
      "\"cases\", \"CONSTRAINT ON ( cases:Cases ) ASSERT (cases.id) IS UNIQUE\", \"Constraint( id=234, name='cases', type='UNIQUENESS', schema=(:Cases {id}), ownedIndex=233 )\"\n",
      "\"chain\", \"CONSTRAINT ON ( chain:Chain ) ASSERT (chain.id) IS UNIQUE\", \"Constraint( id=239, name='chain', type='UNIQUENESS', schema=(:Chain {id}), ownedIndex=238 )\"\n",
      "\"city\", \"CONSTRAINT ON ( city:City ) ASSERT (city.id) IS UNIQUE\", \"Constraint( id=196, name='city', type='UNIQUENESS', schema=(:City {id}), ownedIndex=195 )\"\n",
      "\"commuting\", \"CONSTRAINT ON ( commuting:Commuting ) ASSERT (commuting.id) IS UNIQUE\", \"Constraint( id=287, name='commuting', type='UNIQUENESS', schema=(:Commuting {id}), ownedIndex=286 )\"\n",
      "\"computers\", \"CONSTRAINT ON ( computers:Computers ) ASSERT (computers.id) IS UNIQUE\", \"Constraint( id=257, name='computers', type='UNIQUENESS', schema=(:Computers {id}), ownedIndex=256 )\"\n",
      "\"country\", \"CONSTRAINT ON ( country:Country ) ASSERT (country.id) IS UNIQUE\", \"Constraint( id=173, name='country', type='UNIQUENESS', schema=(:Country {id}), ownedIndex=172 )\"\n",
      "\"cruiseship\", \"CONSTRAINT ON ( cruiseship:CruiseShip ) ASSERT (cruiseship.id) IS UNIQUE\", \"Constraint( id=208, name='cruiseship', type='UNIQUENESS', schema=(:CruiseShip {id}), ownedIndex=207 )\"\n",
      "\"demographics\", \"CONSTRAINT ON ( demographics:Demographics ) ASSERT (demographics.id) IS UNIQUE\", \"Constraint( id=311, name='demographics', type='UNIQUENESS', schema=(:Demographics {id}), ownedIndex=310 )\"\n",
      "\"economics\", \"CONSTRAINT ON ( economics:Economics ) ASSERT (economics.id) IS UNIQUE\", \"Constraint( id=263, name='economics', type='UNIQUENESS', schema=(:Economics {id}), ownedIndex=262 )\"\n",
      "\"education\", \"CONSTRAINT ON ( education:Education ) ASSERT (education.id) IS UNIQUE\", \"Constraint( id=251, name='education', type='UNIQUENESS', schema=(:Education {id}), ownedIndex=250 )\"\n",
      "\"employment\", \"CONSTRAINT ON ( employment:Employment ) ASSERT (employment.id) IS UNIQUE\", \"Constraint( id=269, name='employment', type='UNIQUENESS', schema=(:Employment {id}), ownedIndex=268 )\"\n",
      "\"gene\", \"CONSTRAINT ON ( gene:Gene ) ASSERT (gene.id) IS UNIQUE\", \"Constraint( id=223, name='gene', type='UNIQUENESS', schema=(:Gene {id}), ownedIndex=222 )\"\n",
      "\"healthinsurance\", \"CONSTRAINT ON ( healthinsurance:HealthInsurance ) ASSERT (healthinsurance.id) IS UNIQUE\", \"Constraint( id=281, name='healthinsurance', type='UNIQUENESS', schema=(:HealthInsurance {id}), ownedIndex=280 )\"\n",
      "\"housing\", \"CONSTRAINT ON ( housing:Housing ) ASSERT (housing.id) IS UNIQUE\", \"Constraint( id=305, name='housing', type='UNIQUENESS', schema=(:Housing {id}), ownedIndex=304 )\"\n",
      "\"income\", \"CONSTRAINT ON ( income:Income ) ASSERT (income.id) IS UNIQUE\", \"Constraint( id=275, name='income', type='UNIQUENESS', schema=(:Income {id}), ownedIndex=274 )\"\n",
      "\"location\", \"CONSTRAINT ON ( location:Location ) ASSERT (location.id) IS UNIQUE\", \"Constraint( id=161, name='location', type='UNIQUENESS', schema=(:Location {id}), ownedIndex=160 )\"\n",
      "\"occupation\", \"CONSTRAINT ON ( occupation:Occupation ) ASSERT (occupation.id) IS UNIQUE\", \"Constraint( id=293, name='occupation', type='UNIQUENESS', schema=(:Occupation {id}), ownedIndex=292 )\"\n",
      "\"organism\", \"CONSTRAINT ON ( organism:Organism ) ASSERT (organism.id) IS UNIQUE\", \"Constraint( id=211, name='organism', type='UNIQUENESS', schema=(:Organism {id}), ownedIndex=210 )\"\n",
      "\"outbreak\", \"CONSTRAINT ON ( outbreak:Outbreak ) ASSERT (outbreak.id) IS UNIQUE\", \"Constraint( id=213, name='outbreak', type='UNIQUENESS', schema=(:Outbreak {id}), ownedIndex=212 )\"\n",
      "\"postalcode\", \"CONSTRAINT ON ( postalcode:PostalCode ) ASSERT (postalcode.id) IS UNIQUE\", \"Constraint( id=201, name='postalcode', type='UNIQUENESS', schema=(:PostalCode {id}), ownedIndex=200 )\"\n",
      "\"poverty\", \"CONSTRAINT ON ( poverty:Poverty ) ASSERT (poverty.id) IS UNIQUE\", \"Constraint( id=299, name='poverty', type='UNIQUENESS', schema=(:Poverty {id}), ownedIndex=298 )\"\n",
      "\"protein\", \"CONSTRAINT ON ( protein:Protein ) ASSERT (protein.id) IS UNIQUE\", \"Constraint( id=228, name='protein', type='UNIQUENESS', schema=(:Protein {id}), ownedIndex=227 )\"\n",
      "\"proteinname\", \"CONSTRAINT ON ( proteinname:ProteinName ) ASSERT (proteinname.id) IS UNIQUE\", \"Constraint( id=230, name='proteinname', type='UNIQUENESS', schema=(:ProteinName {id}), ownedIndex=229 )\"\n",
      "\"publication\", \"CONSTRAINT ON ( publication:Publication ) ASSERT (publication.id) IS UNIQUE\", \"Constraint( id=215, name='publication', type='UNIQUENESS', schema=(:Publication {id}), ownedIndex=214 )\"\n",
      "\"socialcharacteristics\", \"CONSTRAINT ON ( socialcharacteristics:SocialCharacteristics ) ASSERT (socialcharacteristics.id) IS UNIQUE\", \"Constraint( id=245, name='socialcharacteristics', type='UNIQUENESS', schema=(:SocialCharacteristics {id}), ownedIndex=244 )\"\n",
      "\"strain\", \"CONSTRAINT ON ( strain:Strain ) ASSERT (strain.id) IS UNIQUE\", \"Constraint( id=217, name='strain', type='UNIQUENESS', schema=(:Strain {id}), ownedIndex=216 )\"\n",
      "\"structure\", \"CONSTRAINT ON ( structure:Structure ) ASSERT (structure.id) IS UNIQUE\", \"Constraint( id=242, name='structure', type='UNIQUENESS', schema=(:Structure {id}), ownedIndex=241 )\"\n",
      "\"tract\", \"CONSTRAINT ON ( tract:Tract ) ASSERT (tract.id) IS UNIQUE\", \"Constraint( id=206, name='tract', type='UNIQUENESS', schema=(:Tract {id}), ownedIndex=205 )\"\n",
      "\"unintermediateregion\", \"CONSTRAINT ON ( unintermediateregion:UNIntermediateRegion ) ASSERT (unintermediateregion.id) IS UNIQUE\", \"Constraint( id=171, name='unintermediateregion', type='UNIQUENESS', schema=(:UNIntermediateRegion {id}), ownedIndex=170 )\"\n",
      "\"unregion\", \"CONSTRAINT ON ( unregion:UNRegion ) ASSERT (unregion.id) IS UNIQUE\", \"Constraint( id=167, name='unregion', type='UNIQUENESS', schema=(:UNRegion {id}), ownedIndex=166 )\"\n",
      "\"unsubregion\", \"CONSTRAINT ON ( unsubregion:UNSubRegion ) ASSERT (unsubregion.id) IS UNIQUE\", \"Constraint( id=169, name='unsubregion', type='UNIQUENESS', schema=(:UNSubRegion {id}), ownedIndex=168 )\"\n",
      "\"usdivision\", \"CONSTRAINT ON ( usdivision:USDivision ) ASSERT (usdivision.id) IS UNIQUE\", \"Constraint( id=187, name='usdivision', type='UNIQUENESS', schema=(:USDivision {id}), ownedIndex=186 )\"\n",
      "\"usregion\", \"CONSTRAINT ON ( usregion:USRegion ) ASSERT (usregion.id) IS UNIQUE\", \"Constraint( id=185, name='usregion', type='UNIQUENESS', schema=(:USRegion {id}), ownedIndex=184 )\"\n",
      "\"variant\", \"CONSTRAINT ON ( variant:Variant ) ASSERT (variant.id) IS UNIQUE\", \"Constraint( id=221, name='variant', type='UNIQUENESS', schema=(:Variant {id}), ownedIndex=220 )\"\n",
      "\"world\", \"CONSTRAINT ON ( world:World ) ASSERT (world.id) IS UNIQUE\", \"Constraint( id=165, name='world', type='UNIQUENESS', schema=(:World {id}), ownedIndex=164 )\"\n",
      "id, name, state, populationPercent, uniqueness, type, entityType, labelsOrTypes, properties, provider\n",
      "177, \"admin1\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Admin1\"], [\"id\"], \"native-btree-1.0\"\n",
      "180, \"admin1_f\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin1\"], [\"fips\"], \"native-btree-1.0\"\n",
      "182, \"admin1_g\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin1\"], [\"geonameId\"], \"native-btree-1.0\"\n",
      "183, \"admin1_l\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin1\"], [\"location\"], \"native-btree-1.0\"\n",
      "179, \"admin1_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin1\"], [\"name\"], \"native-btree-1.0\"\n",
      "181, \"admin1_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin1\"], [\"parentId\"], \"native-btree-1.0\"\n",
      "188, \"admin2\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Admin2\"], [\"id\"], \"native-btree-1.0\"\n",
      "190, \"admin2_f\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin2\"], [\"fips\"], \"native-btree-1.0\"\n",
      "192, \"admin2_g\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin2\"], [\"geonameId\"], \"native-btree-1.0\"\n",
      "194, \"admin2_l\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin2\"], [\"location\"], \"native-btree-1.0\"\n",
      "193, \"admin2_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin2\"], [\"name\"], \"native-btree-1.0\"\n",
      "191, \"admin2_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Admin2\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "317, \"bioentities\", \"POPULATING\", 0.0, \"NONUNIQUE\", \"FULLTEXT\", \"NODE\", [\"ProteinName\", \"Protein\", \"Gene\", \"Strain\", \"Variant\", \"Organism\", \"Outbreak\", \"Chain\", \"Structure\"], [\"name\", \"scientificName\", \"taxonomyId\", \"accession\", \"proId\", \"genomeAccession\", \"geneVariant\", \"proteinVariant\", \"variantType\", \"variantConsequence\"], \"fulltext-1.0\"\n",
      "233, \"cases\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Cases\"], [\"id\"], \"native-btree-1.0\"\n",
      "235, \"cases_d\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Cases\"], [\"date\"], \"native-btree-1.0\"\n",
      "237, \"cases_o\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Cases\"], [\"origLocation\"], \"native-btree-1.0\"\n",
      "236, \"cases_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Cases\"], [\"source\"], \"native-btree-1.0\"\n",
      "238, \"chain\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Chain\"], [\"id\"], \"native-btree-1.0\"\n",
      "240, \"chain_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Chain\"], [\"name\"], \"native-btree-1.0\"\n",
      "195, \"city\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"City\"], [\"id\"], \"native-btree-1.0\"\n",
      "199, \"city_g\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"City\"], [\"geonameId\"], \"native-btree-1.0\"\n",
      "198, \"city_l\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"City\"], [\"location\"], \"native-btree-1.0\"\n",
      "197, \"city_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"City\"], [\"name\"], \"native-btree-1.0\"\n",
      "286, \"commuting\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Commuting\"], [\"id\"], \"native-btree-1.0\"\n",
      "288, \"commuting_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Commuting\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "290, \"commuting_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Commuting\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "289, \"commuting_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Commuting\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "291, \"commuting_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Commuting\"], [\"tract\"], \"native-btree-1.0\"\n",
      "256, \"computers\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Computers\"], [\"id\"], \"native-btree-1.0\"\n",
      "258, \"computers_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Computers\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "260, \"computers_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Computers\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "259, \"computers_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Computers\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "261, \"computers_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Computers\"], [\"tract\"], \"native-btree-1.0\"\n",
      "172, \"country\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Country\"], [\"id\"], \"native-btree-1.0\"\n",
      "175, \"country_g\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Country\"], [\"geonameId\"], \"native-btree-1.0\"\n",
      "176, \"country_l\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Country\"], [\"location\"], \"native-btree-1.0\"\n",
      "174, \"country_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Country\"], [\"name\"], \"native-btree-1.0\"\n",
      "207, \"cruiseship\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"CruiseShip\"], [\"id\"], \"native-btree-1.0\"\n",
      "209, \"cruiseship_o\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"CruiseShip\"], [\"origLocation\"], \"native-btree-1.0\"\n",
      "310, \"demographics\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Demographics\"], [\"id\"], \"native-btree-1.0\"\n",
      "312, \"demographics_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Demographics\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "314, \"demographics_p\", \"POPULATING\", 66.5999984741211, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Demographics\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "313, \"demographics_s\", \"POPULATING\", 66.5999984741211, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Demographics\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "315, \"demographics_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Demographics\"], [\"tract\"], \"native-btree-1.0\"\n",
      "262, \"economics\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Economics\"], [\"id\"], \"native-btree-1.0\"\n",
      "264, \"economics_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Economics\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "266, \"economics_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Economics\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "265, \"economics_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Economics\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "267, \"economics_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Economics\"], [\"tract\"], \"native-btree-1.0\"\n",
      "250, \"education\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Education\"], [\"id\"], \"native-btree-1.0\"\n",
      "252, \"education_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Education\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "254, \"education_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Education\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "253, \"education_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Education\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "255, \"education_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Education\"], [\"tract\"], \"native-btree-1.0\"\n",
      "268, \"employment\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Employment\"], [\"id\"], \"native-btree-1.0\"\n",
      "270, \"employment_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Employment\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "272, \"employment_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Employment\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "271, \"employment_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Employment\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "273, \"employment_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Employment\"], [\"tract\"], \"native-btree-1.0\"\n",
      "222, \"gene\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Gene\"], [\"id\"], \"native-btree-1.0\"\n",
      "224, \"gene_a\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Gene\"], [\"genomeAccession\"], \"native-btree-1.0\"\n",
      "226, \"gene_e\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Gene\"], [\"end\"], \"native-btree-1.0\"\n",
      "225, \"gene_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Gene\"], [\"start\"], \"native-btree-1.0\"\n",
      "318, \"geoids\", \"POPULATING\", 0.0, \"NONUNIQUE\", \"FULLTEXT\", \"NODE\", [\"UNRegion\", \"UNSubRegion\", \"UNIntermediateRegion\", \"Country\", \"Admin1\", \"Admin2\", \"USRegion\", \"USDivision\", \"City\", \"PostalCode\", \"Tract\"], [\"id\", \"iso\", \"iso3\", \"fips\", \"geonameId\", \"code\", \"name\"], \"fulltext-1.0\"\n",
      "280, \"healthinsurance\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"HealthInsurance\"], [\"id\"], \"native-btree-1.0\"\n",
      "282, \"healthinsurance_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"HealthInsurance\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "284, \"healthinsurance_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"HealthInsurance\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "283, \"healthinsurance_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"HealthInsurance\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "285, \"healthinsurance_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"HealthInsurance\"], [\"tract\"], \"native-btree-1.0\"\n",
      "304, \"housing\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Housing\"], [\"id\"], \"native-btree-1.0\"\n",
      "306, \"housing_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Housing\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "308, \"housing_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Housing\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "307, \"housing_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Housing\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "309, \"housing_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Housing\"], [\"tract\"], \"native-btree-1.0\"\n",
      "274, \"income\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Income\"], [\"id\"], \"native-btree-1.0\"\n",
      "276, \"income_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Income\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "278, \"income_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Income\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "277, \"income_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Income\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "279, \"income_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Income\"], [\"tract\"], \"native-btree-1.0\"\n",
      "160, \"location\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Location\"], [\"id\"], \"native-btree-1.0\"\n",
      "163, \"location_l\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Location\"], [\"location\"], \"native-btree-1.0\"\n",
      "162, \"location_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Location\"], [\"name\"], \"native-btree-1.0\"\n",
      "316, \"locations\", \"POPULATING\", 100.0, \"NONUNIQUE\", \"FULLTEXT\", \"NODE\", [\"World\", \"UNRegion\", \"UNSubRegion\", \"UNIntermediateRegion\", \"Country\", \"Admin1\", \"Admin2\", \"USRegion\", \"USDivision\", \"City\", \"CruiseShip\", \"PostalCode\", \"Tract\"], [\"name\", \"placeName\", \"iso\", \"iso3\", \"fips\", \"geonameId\", \"code\", \"origLocation\"], \"fulltext-1.0\"\n",
      "292, \"occupation\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Occupation\"], [\"id\"], \"native-btree-1.0\"\n",
      "294, \"occupation_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Occupation\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "296, \"occupation_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Occupation\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "295, \"occupation_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Occupation\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "297, \"occupation_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Occupation\"], [\"tract\"], \"native-btree-1.0\"\n",
      "210, \"organism\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Organism\"], [\"id\"], \"native-btree-1.0\"\n",
      "212, \"outbreak\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Outbreak\"], [\"id\"], \"native-btree-1.0\"\n",
      "200, \"postalcode\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"PostalCode\"], [\"id\"], \"native-btree-1.0\"\n",
      "204, \"postalcode_l\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"PostalCode\"], [\"location\"], \"native-btree-1.0\"\n",
      "202, \"postalcode_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"PostalCode\"], [\"name\"], \"native-btree-1.0\"\n",
      "203, \"postalcode_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"PostalCode\"], [\"placeName\"], \"native-btree-1.0\"\n",
      "298, \"poverty\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Poverty\"], [\"id\"], \"native-btree-1.0\"\n",
      "300, \"poverty_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Poverty\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "302, \"poverty_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Poverty\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "301, \"poverty_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Poverty\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "303, \"poverty_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Poverty\"], [\"tract\"], \"native-btree-1.0\"\n",
      "227, \"protein\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Protein\"], [\"id\"], \"native-btree-1.0\"\n",
      "229, \"proteinname\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"ProteinName\"], [\"id\"], \"native-btree-1.0\"\n",
      "232, \"proteinname_a\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"ProteinName\"], [\"accession\"], \"native-btree-1.0\"\n",
      "231, \"proteinname_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"ProteinName\"], [\"name\"], \"native-btree-1.0\"\n",
      "214, \"publication\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Publication\"], [\"id\"], \"native-btree-1.0\"\n",
      "244, \"socialcharacteristics\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"SocialCharacteristics\"], [\"id\"], \"native-btree-1.0\"\n",
      "246, \"socialcharacteristics_c\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"SocialCharacteristics\"], [\"countyFips\"], \"native-btree-1.0\"\n",
      "248, \"socialcharacteristics_p\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"SocialCharacteristics\"], [\"postalCode\"], \"native-btree-1.0\"\n",
      "247, \"socialcharacteristics_s\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"SocialCharacteristics\"], [\"stateFips\"], \"native-btree-1.0\"\n",
      "249, \"socialcharacteristics_t\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"SocialCharacteristics\"], [\"tract\"], \"native-btree-1.0\"\n",
      "216, \"strain\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Strain\"], [\"id\"], \"native-btree-1.0\"\n",
      "218, \"strain_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Strain\"], [\"name\"], \"native-btree-1.0\"\n",
      "219, \"strain_o\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Strain\"], [\"origLocation\"], \"native-btree-1.0\"\n",
      "241, \"structure\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Structure\"], [\"id\"], \"native-btree-1.0\"\n",
      "243, \"structure_n\", \"ONLINE\", 100.0, \"NONUNIQUE\", \"BTREE\", \"NODE\", [\"Structure\"], [\"name\"], \"native-btree-1.0\"\n",
      "205, \"tract\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Tract\"], [\"id\"], \"native-btree-1.0\"\n",
      "170, \"unintermediateregion\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"UNIntermediateRegion\"], [\"id\"], \"native-btree-1.0\"\n",
      "166, \"unregion\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"UNRegion\"], [\"id\"], \"native-btree-1.0\"\n",
      "168, \"unsubregion\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"UNSubRegion\"], [\"id\"], \"native-btree-1.0\"\n",
      "186, \"usdivision\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"USDivision\"], [\"id\"], \"native-btree-1.0\"\n",
      "184, \"usregion\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"USRegion\"], [\"id\"], \"native-btree-1.0\"\n",
      "220, \"variant\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"Variant\"], [\"id\"], \"native-btree-1.0\"\n",
      "164, \"world\", \"ONLINE\", 100.0, \"UNIQUE\", \"BTREE\", \"NODE\", [\"World\"], [\"id\"], \"native-btree-1.0\"\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00b-Organism.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///Organism.csv' AS row \n",
      "WITH row WHERE row.type='Host'\n",
      "MERGE (h:Host{id: row.id})\n",
      "SET h.name = row.name, h.scientificName = row.scientificName\n",
      "RETURN count(h) as Host\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///Organism.csv' AS row \n",
      "WITH row WHERE row.type='Pathogen'\n",
      "MERGE (p:Pathogen{id: row.id})\n",
      "SET p.name = row.name, p.scientificName = row.scientificName\n",
      "RETURN count(p) as Pathogen\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///Organism.csv' AS row \n",
      "MERGE (o:Organism{id: row.id})\n",
      "SET o.name = row.name, o.scientificName = row.scientificName, o.type = row.type\n",
      "RETURN count(o) as Organism\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///Organism.csv' AS row \n",
      "WITH row WHERE row.type='Host'\n",
      "MATCH (h:Host{id: row.id})\n",
      "MATCH (o:Organism{id: row.id})\n",
      "MERGE (h)-[i:IS_A]->(o)\n",
      "RETURN count(i) as IS_A\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///Organism.csv' AS row \n",
      "WITH row WHERE row.type='Pathogen'\n",
      "MATCH (p:Pathogen{id: row.id})\n",
      "MATCH (o:Organism{id: row.id})\n",
      "MERGE (p)-[i:IS_A]->(o)\n",
      "RETURN count(i) as IS_A\n",
      ";\n",
      "\n",
      "Host\n",
      "12\n",
      "Pathogen\n",
      "3\n",
      "Organism\n",
      "15\n",
      "IS_A\n",
      "12\n",
      "IS_A\n",
      "3\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00c-Outbreak.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///Outbreak.csv' AS row \n",
      "MERGE (o:Outbreak{id: row.id})\n",
      "SET o.name = row.id, o.startDate = row.startDate, o.pathogen = row.pathogen\n",
      "RETURN count(o) as Outbreak\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///Outbreak.csv' AS row \n",
      "MATCH (p:Pathogen{id: row.pathogen})\n",
      "MATCH (o:Outbreak{id: row.id})\n",
      "MERGE(p)-[c:CAUSES]->(o)\n",
      "RETURN count(c) as CAUSES\n",
      ";\n",
      "                    \n",
      "Outbreak\n",
      "3\n",
      "CAUSES\n",
      "3\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00e-GeoNamesCountry.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00e-GeoNamesCountry.csv' AS row \n",
      "MERGE (c:Country:Location{id: row.id})\n",
      "SET c.name = row.name, c.iso = row.iso, c.iso3 = row.iso3, c.isoNumeric = row.isoNumeric, c.geonameId = row.geonameId, c.areaSqKm = toInteger(row.areaSqKm), c.population = toInteger(row.population),\n",
      "c.location = point({longitude: toFloat(row.longitude), latitude: toFloat(row.latitude)})\n",
      "RETURN count(c) as Country\n",
      ";\n",
      "Country\n",
      "252\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00f-GeoNamesAdmin1.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00f-GeoNamesAdmin1.csv' AS row \n",
      "MERGE (a:Admin1:Location{id: row.id})\n",
      "SET a.name = row.name, a.code = row.code, a.country = row.parentId, a.geonameId = row.geonameId, a.population = toInteger(row.population), a.elevation = toInteger(row.elevation),\n",
      "a.location = point({longitude: toFloat(row.longitude), latitude: toFloat(row.latitude)})\n",
      "RETURN count(a) as Admin1\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00f-GeoNamesAdmin1.csv' AS row \n",
      "MATCH (a:Admin1{id: row.id})\n",
      "MATCH (ct:Country{id: row.parentId})\n",
      "MERGE (a)-[i:IN]->(ct)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "Admin1\n",
      "3934\n",
      "IN\n",
      "3920\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00g-GeoNamesAdmin2.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT 500\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00g-GeoNamesAdmin2.csv' AS row \n",
      "MERGE (a:Admin2:Location{id: row.id})\n",
      "SET a.name = row.name, a.parentId = row.parentId, a.geonameId = row.geonameId, a.population = toInteger(row.population), a.elevation = toInteger(row.elevation),\n",
      "a.location = point({longitude: toFloat(row.longitude), latitude: toFloat(row.latitude)})\n",
      "RETURN count(a) as Admin2\n",
      ";\n",
      "USING PERIODIC COMMIT 500\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00g-GeoNamesAdmin2.csv' AS row \n",
      "MATCH (a2:Admin2{id: row.id})\n",
      "MATCH (a1:Admin1{id: row.parentId})\n",
      "MERGE (a2)-[i:IN]->(a1)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "Admin2\n",
      "44779\n",
      "IN\n",
      "44759\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00h-GeoNamesCity.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT 500\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00h-GeoNamesCity.csv' AS row \n",
      "MERGE (c:City:Location{id: row.id})\n",
      "SET c.name = row.name, c.geonameId = row.geonameId, c.population = toInteger(row.population), c.elevation = toInteger(row.elevation),\n",
      "c.location = point({longitude: toFloat(row.longitude), latitude: toFloat(row.latitude)})\n",
      "RETURN count(c) as City\n",
      ";\n",
      "USING PERIODIC COMMIT 500\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00h-GeoNamesCity.csv' AS row \n",
      "MATCH (l:Location{id: row.parentId})\n",
      "MATCH (c:City{id: row.id})\n",
      "MERGE (c)-[i:IN]->(l)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "City\n",
      "137497\n",
      "IN\n",
      "133546\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00i-USCensusRegionDivisionState2017.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00i-USCensus2017Region.csv' AS row \n",
      "MERGE (r:USRegion:Location{id: row.id})\n",
      "SET r.name = row.name\n",
      "RETURN count(r) as Region\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00i-USCensus2017Division.csv' AS row \n",
      "MERGE (d:USDivision:Location{id: row.id})\n",
      "SET d.name = row.name\n",
      "RETURN count(d) as Division\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00i-USCensus2017State.csv' AS row \n",
      "MATCH (a:Admin1{name: row.name, country: 'US'})\n",
      "SET a.fips = row.fips, a.division = row.division\n",
      "RETURN count(a) as FIPS\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00i-USCensus2017Region.csv' AS row \n",
      "MATCH (r:USRegion{id: row.id})\n",
      "MATCH (c:Country{id: row.parentId})\n",
      "MERGE (r)-[i:IN]->(c)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00i-USCensus2017Division.csv' AS row \n",
      "MATCH (d:USDivision{id: row.id})\n",
      "MATCH (r:USRegion{id: row.parentId})\n",
      "MERGE (d)-[i:IN]->(r)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00i-USCensus2017State.csv' AS row \n",
      "MATCH (a:Admin1{name: row.name, country: 'US'})\n",
      "MATCH (d:USDivision{id: row.parentId})\n",
      "MERGE (a)-[i:IN]->(d)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "\n",
      "Region\n",
      "4\n",
      "Division\n",
      "9\n",
      "FIPS\n",
      "51\n",
      "IN\n",
      "4\n",
      "IN\n",
      "9\n",
      "IN\n",
      "51\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00j-USCensusCountyCity2017.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00j-USCensus2017County.csv' AS row \n",
      "MATCH (a:Admin2)-[:IN]->(a1:Admin1{fips: row.stateFips})\n",
      "WHERE a.id STARTS WITH 'US.' AND a.id ENDS WITH row.fips\n",
      "SET a.fips = row.fips, a.stateFips = row.stateFips\n",
      "RETURN count(a) as Admin2_Fips\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00j-USCensus2017City.csv' AS row \n",
      "MATCH (c:City{name: row.name})-[:IN]->(:Admin2)-[:IN]->(a1:Admin1{fips: row.stateFips})\n",
      "SET c.fips = row.fips, c.stateFips = row.stateFips\n",
      "RETURN count(c) as City_Fips\n",
      ";\n",
      "Admin2_Fips\n",
      "3142\n",
      "City_Fips\n",
      "8356\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00k-UNRegion.cypher:\n",
      " \n",
      "MERGE (w:World:Location{id: \"m49:1\"})\n",
      "SET w.name = \"World\"\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00k-UNAll.csv' AS row\n",
      "MERGE (r:UNRegion:Location{id: row.UNRegionCode})\n",
      "SET r.name = row.UNRegion\n",
      "RETURN count(r) as UNRegion\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00k-UNAll.csv' AS row \n",
      "WITH row WHERE NOT row.UNSubRegion IS null\n",
      "MERGE (s:UNSubRegion:Location{id: row.UNSubRegionCode})\n",
      "SET s.name = row.UNSubRegion\n",
      "RETURN count(s) as UNSubRegion\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00k-UNAll.csv' AS row\n",
      "WITH row WHERE NOT row.UNIntermediateRegion IS null\n",
      "MERGE (n:UNIntermediateRegion:Location{id: row.UNIntermediateRegionCode})\n",
      "SET n.name = row.UNIntermediateRegion\n",
      "RETURN count(n) as UNIntermediateRegion\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00k-UNAll.csv' AS row \n",
      "MATCH (r:UNRegion{id: row.UNRegionCode})\n",
      "MATCH (w:World{id: \"m49:1\"})\n",
      "MERGE (r)-[i:IN]->(w)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00k-UNAll.csv' AS row\n",
      "WITH row WHERE NOT row.UNSubRegion IS null\n",
      "MATCH (s:UNSubRegion{id: row.UNSubRegionCode})\n",
      "MATCH (r:UNRegion{id: row.UNRegionCode})\n",
      "MERGE (s)-[i:IN]->(r)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00k-UNAll.csv' AS row \n",
      "WITH row WHERE NOT row.UNIntermediateRegion IS null\n",
      "MATCH (n:UNIntermediateRegion{id: row.UNIntermediateRegionCode})\n",
      "MATCH (s:UNSubRegion{id: row.UNSubRegionCode})\n",
      "MERGE (n)-[i:IN]->(s)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00k-UNRegion.csv' AS row \n",
      "MATCH (c:Country{iso3: row.iso3})\n",
      "MATCH (r:UNRegion{id: row.UNRegionCode})\n",
      "MERGE (c)-[i:IN]->(r)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00k-UNSubRegion.csv' AS row \n",
      "MATCH (c:Country{iso3: row.iso3})\n",
      "MATCH (s:UNSubRegion{id: row.UNSubRegionCode})\n",
      "MERGE (c)-[i:IN]->(s)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00k-UNIntermediateRegion.csv' AS row \n",
      "MATCH (c:Country{iso3: row.iso3})\n",
      "MATCH (n:UNIntermediateRegion{id: row.UNIntermediateRegionCode})\n",
      "MERGE(c)-[i:IN]->(n)\n",
      "RETURN count(i) as IN\n",
      ";\n",
      "                                        \n",
      "\n",
      "UNRegion\n",
      "255\n",
      "UNSubRegion\n",
      "254\n",
      "UNIntermediateRegion\n",
      "107\n",
      "IN\n",
      "255\n",
      "IN\n",
      "254\n",
      "IN\n",
      "107\n",
      "IN\n",
      "1\n",
      "IN\n",
      "146\n",
      "IN\n",
      "107\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00m-USHUDCrosswalk.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00m-USHUDCrosswalkZipToCounty2020Q1.csv' AS row\n",
      "// adding zip to avoid conflicts with other location ids\n",
      "MERGE (p:PostalCode:Location{id: 'zip' + row.zip})\n",
      "SET p.name = row.zip\n",
      "RETURN count(p) as PostalCode\n",
      ";\n",
      "// Note, one Zip code can overlap with multiple counties (Admin2s), e.g. Zip 21771 -> 4 counties\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00m-USHUDCrosswalkZipToCounty2020Q1.csv' AS row\n",
      "MATCH (p:PostalCode{id: 'zip' + row.zip})\n",
      "MATCH (a:Admin2{fips: row.countyFips, stateFips: row.stateFips})\n",
      "MERGE (p)-[i:IN{id: 'zip_to_admin2-' + row.zip + '-' + row.countyFips}]->(a)\n",
      "SET i.resRatio = toFloat(row.resRatio), i.busRatio = toFloat(row.busRatio), i.othRatio = toFloat(row.othRatio), i.totRatio = toFloat(row.totRatio)\n",
      "RETURN count(i) as IN_ADMIN2\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00m-USHUDCrosswalkZipToTract2020Q1.csv' AS row\n",
      "// adding tract to avoid conflicts with other location ids\n",
      "MERGE (t:Tract:Location{id: 'tract' + row.tract})\n",
      "SET t.name = row.tract\n",
      "RETURN count(t) as Tract\n",
      ";\n",
      "// Most tracts are defined in 00m-USHUDCrosswalkZipToTract2020Q1.csv, but not all tracts match the ACS 5-year dataset.\n",
      "// Here we add any missing tracts from the ACS 5-year dataset\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00m-USHUDCrosswalkTractToCounty.csv' AS row\n",
      "MERGE (t:Tract:Location{id: 'tract' + row.tract})\n",
      "SET t.name = row.tract\n",
      "RETURN count(t) as Tract\n",
      ";\n",
      "// Note, one Zip code can overlap with multiple Census Tracts\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00m-USHUDCrosswalkZipToTract2020Q1.csv' AS row\n",
      "MATCH (p:PostalCode{id: 'zip' + row.zip})\n",
      "MATCH (t:Tract{id: 'tract' + row.tract})\n",
      "//MERGE (t)-[i:IN]->(p)\n",
      "// Adding these properties on the server takes too long (never completes?)\n",
      "MERGE (t)-[i:IN{id: 'zip_to_tract-' + row.zip + '-' + row.tract}]->(p)\n",
      "// Adding these properties on the server takes too long (never completes?)\n",
      "SET i.resRatio = toFloat(row.resRatio), i.busRatio = toFloat(row.busRatio), i.othRatio = toFloat(row.othRatio), i.totRatio = toFloat(row.totRatio)\n",
      "RETURN count(i) as IN_TRACT\n",
      ";\n",
      "// Most tracts are defined in 00m-USHUDCrosswalkZipToTract2020Q1.csv, but not all tracts match the ACS 5-year dataset.\n",
      "// Here we add any missing tracts from the ACS 5-year dataset\n",
      "//USING PERIODIC COMMIT\n",
      "//LOAD CSV WITH HEADERS \n",
      "//FROM 'FILE:///00m-USHUDCrosswalkTractToCounty.csv' AS row\n",
      "//MERGE (t:Tract:Location{id: 'tract' + row.tract})\n",
      "//SET t.name = row.tract\n",
      "//RETURN count(t) as Tract\n",
      "//;\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00m-USHUDCrosswalkTractToCounty.csv' AS row\n",
      "MATCH (t:Tract{id: 'tract' + row.tract})\n",
      "MATCH (a:Admin2{fips: row.countyFips, stateFips: row.stateFips})\n",
      "MERGE (t)-[i:IN]->(a)\n",
      "RETURN count(i) as IN_Admin2\n",
      ";\n",
      "\n",
      "                         \n",
      "\n",
      "\n",
      "PostalCode\n",
      "54181\n",
      "IN_ADMIN2\n",
      "53823\n",
      "Tract\n",
      "172121\n",
      "Tract\n",
      "73056\n",
      "IN_TRACT\n",
      "172121\n",
      "IN_Admin2\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00o-GeoNamesPostalCode.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00o-GeoNamesPostalCode.csv' AS row\n",
      "// adding zip to avoid conflicts with other location ids\n",
      "MERGE (p:PostalCode:Location{id: 'zip' + row.zip})\n",
      "SET p.name = row.zip, p.placeName = row.placeName, p.location = point({longitude: toFloat(row.longitude), latitude: toFloat(row.latitude), crs: 'WGS-84'})\n",
      "RETURN count(p) as PostalCode\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///00o-GeoNamesPostalCode.csv' AS row\n",
      "MATCH (p:PostalCode{id: 'zip' + row.zip})\n",
      "MATCH (a:Admin2{id: row.admin2_id})\n",
      "MERGE (p)-[i:IN]->(a)\n",
      "RETURN count(i) as IN_ADMIN2\n",
      ";PostalCode\n",
      "41469\n",
      "IN_ADMIN2\n",
      "40936\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00x-NodeMetadata.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///NodeMetadata.csv' AS row \n",
      "MERGE (n:NodeMetadata{name: row.name})\n",
      "SET n.shortDescription = row.shortDescription, n.description = row.description, n.example = row.example, n.definitionSource = row.definitionSource, n.dataProviders = split(row.dataProviders,\";\")\n",
      "RETURN count(n) as NodeMetadata\n",
      ";\n",
      "NodeMetadata\n",
      "40\n",
      " \n",
      "----------------------------------------------\n",
      "Running 00y-DataProvider.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///DataProvider.csv' AS row \n",
      "MERGE (d:DataProvider{id: row.id})\n",
      "SET d.id = row.id, d.name = row.name, d.url = row.url, d.license = row.license\n",
      "RETURN count(d) as DataProvider\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///NodeMetadata.csv' AS row \n",
      "UNWIND split(row.dataProviders, ';') AS dp\n",
      "MATCH (d:DataProvider{id: dp})\n",
      "MATCH (n:NodeMetadata{name: row.name})\n",
      "MERGE (n)-[u:USES_DATA_FROM]->(d)\n",
      "RETURN count(u) as USES_DATA_FROM\n",
      ";DataProvider\n",
      "16\n",
      "USES_DATA_FROM\n",
      "65\n",
      " \n",
      "----------------------------------------------\n",
      "Running 01a-NCBIStrain.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01a-NCBIStrain.csv' AS row \n",
      "WITH row WHERE NOT row.id IS null\n",
      "MERGE (s:Strain{id: row.id})\n",
      "SET s.name = row.name, s.taxonomyId = row.taxonomy_id, s.collectionDate = date(row.collection_date),\n",
      "    s.hostTaxonomyId = row.host_taxonomy_id, s.sex = row.sex, s.age = toInteger(row.age), \n",
      "    s.isolationSource = row.isolation_source\n",
      "RETURN count(s) as Strain\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01a-NCBIStrain.csv' AS row \n",
      "WITH row WHERE NOT row.taxonomy_id IS null\n",
      "MATCH (p:Pathogen{id: row.taxonomy_id})\n",
      "MATCH (s:Strain{id: row.id})\n",
      "MERGE (p)-[h:HAS_STRAIN]->(s)\n",
      "RETURN count(h) as HAS_STRAIN\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01a-NCBIStrain.csv' AS row \n",
      "WITH row WHERE NOT row.host_taxonomy_id IS null\n",
      "MATCH (h:Host{id: row.host_taxonomy_id})\n",
      "MATCH (s:Strain{id: row.id})\n",
      "MERGE (h)-[c:CARRIES]->(s)\n",
      "RETURN count(c) as CARRIES\n",
      ";\n",
      "                \n",
      "Strain\n",
      "2\n",
      "HAS_STRAIN\n",
      "2\n",
      "CARRIES\n",
      "2\n",
      " \n",
      "----------------------------------------------\n",
      "Running 01b-Nextstrain.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01b-Nextstrain.csv' AS row \n",
      "WITH row WHERE NOT row.id IS null\n",
      "MERGE (s:Strain{id: row.id}) \n",
      "SET s.name = row.name, s.taxonomyId = row.taxonomyId, s.collectionDate = date(row.collectionDate),\n",
      "    s.hostTaxonomyId = row.hostTaxonomyId, s.sex = row.sex, s.age = toInteger(row.age), s.clade = row.clade,\n",
      "    s.exposureCountry = row.exposureCountry, s.exposureAdmin1 = row.exposureAdmin1, s.origLocation = row.origLocation\n",
      "RETURN count(s) as Strain\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01b-Nextstrain.csv' AS row \n",
      "WITH row WHERE NOT row.clade IS null\n",
      "MERGE (c:Clade{id: row.clade})\n",
      "RETURN count(c) as Clade\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01b-Nextstrain.csv' AS row \n",
      "WITH row WHERE NOT row.taxonomyId IS null\n",
      "MATCH (p:Pathogen{id: row.taxonomyId})\n",
      "MATCH (s:Strain{id: row.id})\n",
      "MERGE (p)-[h:HAS_STRAIN]->(s)\n",
      "RETURN count(h) as HAS_STRAIN\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01b-Nextstrain.csv' AS row \n",
      "WITH row WHERE NOT row.clade IS null\n",
      "MATCH (s:Strain{id: row.id})\n",
      "MATCH (c:Clade{id: row.clade})\n",
      "MERGE (s)-[h:HAS_CLADE]->(c)\n",
      "RETURN count(h) as HAS_CLADE\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01b-Nextstrain.csv' AS row \n",
      "WITH row WHERE NOT row.hostTaxonomyId IS null\n",
      "MATCH (h:Host{id: row.hostTaxonomyId})\n",
      "MATCH (s:Strain{id: row.id})\n",
      "MERGE (h)-[c:CARRIES]->(s)\n",
      "RETURN count(c) as CARRIES\n",
      ";\n",
      "Strain\n",
      "4702\n",
      "Clade\n",
      "4702\n",
      "HAS_STRAIN\n",
      "4702\n",
      "HAS_CLADE\n",
      "4702\n",
      "CARRIES\n",
      "4698\n",
      " \n",
      "----------------------------------------------\n",
      "Running 01c-NCBIRefSeq.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01c-NCBIRefSeq.csv' AS row \n",
      "//MERGE (g:Gene{id: row.geneAccession + '-' + row.geneStart + '-' + row.geneEnd})\n",
      "MERGE (g:Gene{id: row.genomeAccession + '-' + row.geneStart + '-' + row.geneEnd})\n",
      "SET g.name = row.geneName, g.accession = row.geneAccession, g.start = toInteger(row.geneStart), g.end = toInteger(row.geneEnd), g.genomeAccession = row.genomeAccession\n",
      "RETURN count(g) as Gene\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01c-NCBIRefSeq.csv' AS row \n",
      "MATCH (gn:Strain{id: row.genomeAccession})\n",
      "//MATCH (g:Gene{id: row.geneAccession + '-' + row.geneStart + '-' + row.geneEnd})\n",
      "MATCH (g:Gene{id: row.genomeAccession + '-' + row.geneStart + '-' + row.geneEnd})\n",
      "MERGE(gn)-[h:HAS]->(g)\n",
      "RETURN count(h) as Has\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01c-NCBIRefSeq.csv' AS row \n",
      "MERGE (p:Protein{id: row.id})\n",
      "SET p.name = row.proteinName, p.accession = row.proteinAccession, p.sequence = row.sequence, p.taxonomyId = row.taxonomyId\n",
      "RETURN count(p) as Protein\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01c-NCBIRefSeq.csv' AS row \n",
      "MERGE (p:ProteinName{id: row.proteinName + row.proteinAccession})\n",
      "SET p.name = row.proteinName, p.accession = row.proteinAccession\n",
      "RETURN count(p) as ProteinName\n",
      ";\n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01c-NCBIRefSeq.csv\" AS row\n",
      "MATCH (p:Protein{id: row.id})\n",
      "MATCH (pn:ProteinName{id: row.proteinName + row.proteinAccession})\n",
      "MERGE (p)-[n:NAMED_AS]->(pn)\n",
      "RETURN count(n) as NAMED_AS\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01c-NCBIRefSeq.csv' AS row \n",
      "//MATCH (g:Gene{id: row.geneAccession + '-' + row.geneStart + '-' + row.geneEnd})\n",
      "MATCH (g:Gene{id: row.genomeAccession + '-' + row.geneStart + '-' + row.geneEnd})\n",
      "MATCH (p:Protein{id: row.id})\n",
      "MERGE(g)-[e:ENCODES]->(p)\n",
      "RETURN count(e) as ENCODES\n",
      ";\n",
      "Gene\n",
      "48\n",
      "Has\n",
      "48\n",
      "Protein\n",
      "48\n",
      "ProteinName\n",
      "48\n",
      "NAMED_AS\n",
      "48\n",
      "ENCODES\n",
      "48\n",
      " \n",
      "----------------------------------------------\n",
      "Running 01d-CNCBStrain.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01d-CNCBStrain.csv' AS row \n",
      "MERGE (s:Strain{id: row.id}) \n",
      "SET s.name = row.name, s.alias = row.alias, s.taxonomyId = row.taxonomyId, s.collectionDate = date(row.collectionDate), s.hostTaxonomyId = row.hostTaxonomyId, s.origLocation = row.origLocation\n",
      "RETURN count(s) as Strain\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01d-CNCBStrain.csv' AS row\n",
      "MATCH (p:Pathogen{id: row.taxonomyId})\n",
      "MATCH (s:Strain{id: row.id})\n",
      "MERGE (p)-[h:HAS_STRAIN]->(s)\n",
      "RETURN count(h) as HAS_STRAIN\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01d-CNCBStrain.csv' AS row\n",
      "MATCH (h:Host{id: row.hostTaxonomyId})\n",
      "MATCH (s:Strain{id: row.id})\n",
      "MERGE (h)-[c:CARRIES]->(s)\n",
      "RETURN count(c) as CARRIES\n",
      ";\n",
      "//USING PERIODIC COMMIT\n",
      "//LOAD CSV WITH HEADERS\n",
      "//FROM 'FILE:///01d-CNCBStrain.csv' AS row\n",
      "//WITH row WHERE row.locationLevels='0'\n",
      "//MATCH (c:Country{name: row.country})\n",
      "//MATCH (s:Strain{id: row.id})\n",
      "//MERGE (s)-[f:FOUND_IN]->(c)\n",
      "//RETURN count(f) as FOUND_IN_COUNTRY\n",
      "//;\n",
      "//USING PERIODIC COMMIT\n",
      "//LOAD CSV WITH HEADERS\n",
      "//FROM 'FILE:///01d-CNCBStrain.csv' AS row\n",
      "//WITH row WHERE row.locationLevels='1'\n",
      "//MATCH (c:Country{name: row.country})<-[:IN*1..2]-(l1:Location{name: row.admin1})\n",
      "//MATCH (s:Strain{id: row.id})\n",
      "//MERGE (s)-[h:FOUND_IN]->(l1)\n",
      "//RETURN count(h) as FOUND_IN_1\n",
      "//;\n",
      "//USING PERIODIC COMMIT\n",
      "//LOAD CSV WITH HEADERS\n",
      "//FROM 'FILE:///01d-CNCBStrain.csv' AS row\n",
      "//WITH row WHERE row.locationLevels='2'\n",
      "//MATCH (c:Country{name: row.country})<-[:IN*1..2]-(l1:Location{name: row.admin1})<-[:IN*1..2]-(l2:Location{name: row.admin2})\n",
      "//MATCH (s:Strain{id: row.id})\n",
      "//MERGE (s)-[h:FOUND_IN]->(l2)\n",
      "//RETURN count(h) as FOUND_IN_2\n",
      "//;\n",
      "//USING PERIODIC COMMIT\n",
      "//LOAD CSV WITH HEADERS\n",
      "//FROM 'FILE:///01d-CNCBStrain.csv' AS row\n",
      "//WITH row WHERE row.locationLevels='3'\n",
      "//MATCH (:Country{name: row.country})<-[:IN]-(:Admin1{name: row.admin1})<-[:IN]-(:Admin2{name: row.admin2})<-[:IN]-(c:City{name: row.city})\n",
      "//MATCH (s:Strain{id: row.id})\n",
      "//MERGE (s)-[h:FOUND_IN]->(c)\n",
      "//RETURN count(h) as FOUND_IN_3\n",
      "//;\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01d-CNCBVariant.csv' AS row\n",
      "MERGE (v:Variant{id: row.referenceGenome + ':' + row.start + '-' + row.end + '-' + row.ref + '-' + row.alt})\n",
      "SET v.name = row.geneVariant, v.geneVariant = row.geneVariant, v.proteinVariant = row.proteinVariant, v.variantType = row.variantType, v.variantConsequence = row.variantConsequence, \n",
      "v.start = toInteger(row.start), v.end = toInteger(row.end), v.ref = row.ref, v.alt = row.alt, \n",
      "v.taxonomyId = row.taxonomyId, v.referenceGenome = row.referenceGenome, v.proteinPosition = toInteger(row.proteinPosition)\n",
      "RETURN count(v) as Variant\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01d-CNCBVariant.csv' AS row\n",
      "MATCH (s:Strain{name: row.name})\n",
      "MATCH (v:Variant{id: row.referenceGenome + ':' + row.start + '-' + row.end + '-' + row.ref + '-' + row.alt})\n",
      "MERGE (s)-[h:HAS_VARIANT]->(v)\n",
      "RETURN count(h) as HAS_VARIANT\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01d-CNCBVariant.csv' AS row\n",
      "// Match GenBank sequences from first SARS-CoV-2 genome (NC_045512, MN908947)\n",
      "MATCH (g:Gene) WHERE toInteger(row.start) >= g.start AND toInteger(row.end) <= g.end \n",
      "                     AND (g.genomeAccession = 'ncbiprotein:NC_045512' OR g.genomeAccession = 'insdc:MN908947')\n",
      "MATCH (v:Variant{id: row.referenceGenome + ':' + row.start + '-' + row.end + '-' + row.ref + '-' + row.alt})\n",
      "MERGE (g)-[h:HAS_VARIANT]->(v)\n",
      "RETURN count(h) as HAS_VARIANT_GENE\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01d-CNCBVariant.csv' AS row\n",
      "MATCH (p:ProteinName{accession: row.proteinAccession})\n",
      "MATCH (v:Variant{id: row.referenceGenome + ':' + row.start + '-' + row.end + '-' + row.ref + '-' + row.alt})\n",
      "MERGE (p)-[h:HAS_VARIANT]->(v)\n",
      "RETURN count(h) as HAS_VARIANT_PROTEIN\n",
      ";\n",
      "\n",
      "                    \n",
      "\n",
      "Strain\n",
      "60126\n",
      "HAS_STRAIN\n",
      "60126\n",
      "CARRIES\n",
      "60107\n",
      "Variant\n",
      "511\n",
      "HAS_VARIANT\n",
      "454\n",
      "HAS_VARIANT_GENE\n",
      "1147\n",
      "HAS_VARIANT_PROTEIN\n",
      "400\n",
      " \n",
      "----------------------------------------------\n",
      "Running 01e-ProteinProteinInteraction.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01e-ProteinProteinInteractionProtein.csv\" AS row\n",
      "MERGE (p:Protein{id: row.id})\n",
      "SET p.name = row.name, p.accession = row.accession, p.proId = row.proId, \n",
      "    p.sequence = row.sequence, p.start = toInteger(row.start), p.end = toInteger(row.end), p.fullLength = row.fullLength, \n",
      "    p.taxonomyId = row.taxonomyId\n",
      "RETURN count(p) as Protein\n",
      ";\n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01e-ProteinProteinInteractionProtein.csv\" AS row\n",
      "MERGE (p:ProteinName{id: coalesce(row.proteinName, '') +  coalesce(row.accession, '') + coalesce(row.proId, '')})\n",
      "SET p.name = row.name, p.accession = row.accession, p.proId = row.proId\n",
      "RETURN count(p) as ProteinName\n",
      ";\n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01e-ProteinProteinInteractionProtein.csv\" AS row\n",
      "MATCH (p1:Protein)\n",
      "MATCH (p2:Protein{id: row.id})\n",
      "WHERE p1.accession = row.accession AND p1.fullLength = 'True' AND p2.fullLength = 'False'\n",
      "MERGE (p1)-[c:CLEAVED_TO]->(p2)\n",
      "RETURN count(c) as CLEAVED_TO\n",
      ";\n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01e-ProteinProteinInteractionProtein.csv\" AS row\n",
      "MATCH (p:Protein{id: row.id})\n",
      "MATCH (pn:ProteinName{id: coalesce(row.proteinName, '') +  coalesce(row.accession, '') + coalesce(row.proId, '')})\n",
      "MERGE (p)-[n:NAMED_AS]->(pn)\n",
      "RETURN count(n) as NAMED_AS\n",
      ";                     \n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01e-ProteinProteinInteraction.csv\" AS row\n",
      "MATCH (pa:Protein{id: row.id_a})\n",
      "MATCH (pb:Protein{id: row.id_b})\n",
      "MERGE (pa)-[i:INTERACTS_WITH]->(pb)\n",
      "RETURN count(i) as INTERACTS_WITH\n",
      ";\n",
      "\n",
      "\n",
      "Protein\n",
      "2073\n",
      "ProteinName\n",
      "2073\n",
      "CLEAVED_TO\n",
      "713\n",
      "NAMED_AS\n",
      "2073\n",
      "INTERACTS_WITH\n",
      "1986\n",
      " \n",
      "----------------------------------------------\n",
      "Running 01f-PDBStructure.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01f-PDBChain.csv\" AS row\n",
      "MERGE (c:Chain{id: row.pdbChainId})\n",
      "SET c.name = row.pdbChainId, c.pdbId = row.pdbId, c.chainId = row.chainId, c.accession = row.accession, \n",
      "    c.uniprotStart = apoc.convert.toIntList(split(row.uniprotStart, ';')), \n",
      "    c.uniprotEnd = apoc.convert.toIntList(split(row.uniprotEnd, ';')),\n",
      "    // pdbStart and pdbEnd contain characters (insertion codes), don't convert to integer list\n",
      "    c.pdbStart = split(row.pdbStart, ';'), c.pdbEnd = split(row.pdbEnd, ';'),\n",
      "    c.seqresStart = apoc.convert.toIntList(split(row.seqresStart, ';')), \n",
      "    c.seqresEnd = apoc.convert.toIntList(split(row.seqresEnd, ';')),\n",
      "    c.residues = toInteger(row.residues)\n",
      "RETURN count(c) as Chain\n",
      ";\n",
      "// Note, PDB assigns chains to the full-length UniProt protein\n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01f-PDBChain.csv\" AS row\n",
      "MATCH (p:Protein{accession: row.accession, fullLength: 'True'})\n",
      "MATCH (c:Chain{id: row.pdbChainId})\n",
      "MERGE (p)-[h:HAS_TERTIARY_STRUCTURE]->(c)\n",
      "SET h.coverage = toFloat(c.residues)/toFloat(size(p.sequence))\n",
      "RETURN count(h) as HAS_TERTIARY_STRUCTURE\n",
      ";\n",
      "//LOAD CSV WITH HEADERS FROM \"FILE:///01f-PDBChain.csv\" AS row\n",
      "//MATCH (p:Protein{accession: row.accession, fullLength: 'True'})\n",
      "//MERGE (c:Chain{id: row.pdbChainId})\n",
      "//SET c.coverage = toFloat(c.residues)/toFloat(size(p.sequence))\n",
      "//RETURN count(c) as COVERAGE\n",
      "//;\n",
      "// Link chains to cleaved proteins if first and last residue is within residue range\n",
      "MATCH (c:Chain)<-[:HAS_TERTIARY_STRUCTURE]-(p:Protein)-[:CLEAVED_TO]->(pc:Protein)\n",
      "WHERE head(c.uniprotStart) >= pc.start AND last(c.uniprotEnd) <= pc.end\n",
      "MERGE (pc)-[h:HAS_TERTIARY_STRUCTURE]->(c)\n",
      "SET h.coverage = toFloat(c.residues)/toFloat(size(pc.sequence))\n",
      "RETURN count(h) as HAS_TERTIARY_STRUCTURE\n",
      ";\n",
      "//MATCH (c:Chain)<-[:HAS_TERTIARY_STRUCTURE]-(p:Protein)-[:CLEAVED_TO]->(pc:Protein)\n",
      "//WHERE head(c.uniprotStart) >= pc.start AND last(c.uniprotEnd) <= pc.end\n",
      "//SET c.coverage = toFloat(c.residues)/toFloat(size(pc.sequence))\n",
      "//RETURN count(c) as COVERAGE\n",
      "//;\n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01f-PDBStructure.csv\" AS row\n",
      "MERGE (s:Structure{id: row.pdbId})\n",
      "SET s.name = row.pdbId, s.title = row.title, s.description = row.description,\n",
      "    s.depositDate = date(row.depositDate), s.releaseDate = date(row.releaseDate),\n",
      "    s.resolution = toFloat(row.resolution), s.rFactor = toFloat(row.rFactor),\n",
      "    s.rFree = toFloat(row.rFree), s.method = row.method\n",
      "RETURN count(s) as Structure\n",
      ";\n",
      "LOAD CSV WITH HEADERS FROM \"FILE:///01f-PDBChain.csv\" AS row\n",
      "MATCH (c:Chain{id: row.pdbChainId})\n",
      "MATCH (s:Structure{id: row.pdbId})\n",
      "MERGE (c)-[h:IS_PART_OF_STRUCTURE]->(s)\n",
      "RETURN count(h) as IS_PART_OF_STRUCTURE    \n",
      ";\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "Chain\n",
      "1255\n",
      "HAS_TERTIARY_STRUCTURE\n",
      "777\n",
      "HAS_TERTIARY_STRUCTURE\n",
      "783\n",
      "Structure\n",
      "371\n",
      "IS_PART_OF_STRUCTURE\n",
      "1255\n",
      " \n",
      "----------------------------------------------\n",
      "Running 01h-PMC-Accession.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01h-PMC-Accession.csv' AS row\n",
      "WITH row WHERE NOT row.accession IS null\n",
      "MERGE (p:Publication{id: row.id})\n",
      "RETURN count(p) as Publication\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01h-PMC-Accession.csv' AS row\n",
      "WITH row WHERE NOT row.accession IS null\n",
      "MATCH (s:Strain{id: row.accession})\n",
      "MATCH (p:Publication{id: row.id})\n",
      "MERGE (p)-[m:MENTIONS]->(s)\n",
      "RETURN count(m) as Publication_Strain\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01h-PMC-Accession.csv' AS row\n",
      "WITH row WHERE NOT row.accession IS null\n",
      "MATCH (n:ProteinName{accession: row.accession})\n",
      "MATCH (p:Publication{id: row.id})\n",
      "MERGE (p)-[m:MENTIONS]->(n)\n",
      "RETURN count(m) as Publication_ProteinName\n",
      ";\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///01h-PMC-Accession.csv' AS row\n",
      "WITH row WHERE NOT row.accession IS null\n",
      "MATCH (s:Structure{id: row.accession})\n",
      "MATCH (p:Publication{id: row.id})\n",
      "MERGE (p)-[m:MENTIONS]->(s)\n",
      "RETURN count(m) as Publication_Structure\n",
      ";Publication\n",
      "23884\n",
      "Publication_Strain\n",
      "943\n",
      "Publication_ProteinName\n",
      "39328\n",
      "Publication_Structure\n",
      "1383\n",
      " \n",
      "----------------------------------------------\n",
      "Running 02a-JHUCases.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02a-JHUCases.csv' AS row \n",
      "MERGE (c:Cases{id: 'COVID-19-JHU' + row.date + '-' + row.stateFips + row.countyFips})\n",
      "SET c.name = 'COVID-19-' + row.date, c.date = date(row.date), c.cases = toInteger(row.cases), c.deaths = toInteger(row.deaths), c.source = 'JHU'\n",
      "RETURN count(c) as CASES\n",
      ";\n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02a-JHUCases.csv' AS row\n",
      "MATCH (c:Cases{id: 'COVID-19-JHU' + row.date + '-' + row.stateFips + row.countyFips})\n",
      "MATCH (a:Admin2{fips: row.countyFips, stateFips: row.stateFips})\n",
      "MERGE (c)-[r:REPORTED_IN]->(a)\n",
      "RETURN count(r) as REPORTED_IN\n",
      ";\n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02a-JHUCases.csv' AS row\n",
      "MATCH (c:Cases{id: 'COVID-19-JHU' + row.date + '-' + row.stateFips + row.countyFips})\n",
      "MATCH (o:Outbreak{id: 'COVID-19'})\n",
      "MERGE (c)-[r:RELATED_TO]->(o)\n",
      "RETURN count(r) as RELATED_TO\n",
      ";\n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02a-JHUCasesGlobal.csv' AS row \n",
      "MERGE (c:Cases{id: 'COVID-19-JHU' + row.date + '-' + row.origLocation})\n",
      "SET c.name = 'COVID-19-' + row.date, c.date = date(row.date), c.cases = toInteger(row.cases), c.deaths = toInteger(row.deaths), c.source = 'JHU',\n",
      "c.origLocation = row.origLocation\n",
      "RETURN count(c) as CASES\n",
      ";\n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02a-JHUCasesGlobal.csv' AS row\n",
      "MATCH (c:Cases{id: 'COVID-19-JHU' + row.date + '-' + row.origLocation})\n",
      "MATCH (o:Outbreak{id: 'COVID-19'})\n",
      "MERGE (c)-[r:RELATED_TO]->(o)\n",
      "RETURN count(r) as RELATED_TO\n",
      ";\n",
      " CASES\n",
      "493067\n",
      "REPORTED_IN\n",
      "478004\n",
      "RELATED_TO\n",
      "493067\n",
      "CASES\n",
      "49538\n",
      "RELATED_TO\n",
      "49538\n",
      " \n",
      "----------------------------------------------\n",
      "Running 02b-CDSCases.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02b-CDSCases.csv' AS row \n",
      "MERGE (c:Cases{id: 'COVID-19-CDS' + row.date + '-' + row.origLocation})\n",
      "SET c.name = 'COVID-19-' + row.date, c.date = date(row.date), c.origLocation = row.origLocation, \n",
      "    c.cases = toInteger(row.cases), c.deaths = toInteger(row.deaths), c.recovered = toInteger(row.recovered), \n",
      "    c.tested = toInteger(row.tested), c.hospitalized = toInteger(row.hospitalized), \n",
      "    c.icu = toInteger(row.icu), c.population = toInteger(row.population),\n",
      "    c.aggregationLevel = row.aggregationLevel,\n",
      "    c.source = 'CDS'\n",
      "RETURN count(c) as CASES\n",
      ";\n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02b-CDSCases.csv' AS row\n",
      "MATCH (c:Cases{id: 'COVID-19-CDS' + row.date + '-' + row.origLocation})\n",
      "MATCH (o:Outbreak{id: 'COVID-19'})\n",
      "MERGE (c)-[r:RELATED_TO]->(o)\n",
      "RETURN count(r) as RELATED_TO\n",
      ";\n",
      "\n",
      " CASES\n",
      "930538\n",
      "RELATED_TO\n",
      "930538\n",
      " \n",
      "----------------------------------------------\n",
      "Running 02c-SDHHSACases.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02c-SDHHSACases.csv' AS row \n",
      "MERGE (c:Cases{id: 'COVID-19-SDHHSA' + row.date + '-' + row.zipCode})\n",
      "SET c.name = 'COVID-19-' + row.date, c.date = date(row.date), c.cases = toInteger(row.cases), c.source = 'SDHHSA'\n",
      "RETURN count(c) as CASES\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02c-SDHHSACases.csv' AS row\n",
      "MATCH (c:Cases{id: 'COVID-19-SDHHSA' + row.date + '-' + row.zipCode})\n",
      "MATCH (p:PostalCode{id: 'zip' + row.zipCode})\n",
      "MERGE (c)-[r:REPORTED_IN]->(p)\n",
      "RETURN count(r) as REPORTED_IN\n",
      ";\n",
      "CASES\n",
      "15521\n",
      "REPORTED_IN\n",
      "15521\n",
      " \n",
      "----------------------------------------------\n",
      "Running 02d-GOBMXCases.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02d-GOBMXCasesAdmin1.csv' AS row \n",
      "MERGE (c:Cases{id: 'COVID-19-GOBMX' + row.date + '-' + row.origLocation})\n",
      "SET c.name = 'COVID-19-' + row.date, c.date = date(row.date), c.cases = toInteger(row.cumulativeCases), \n",
      "    c.deaths = toInteger(row.cumulativeDeaths), c.population = toInteger(row.population), \n",
      "    c.aggregationLevel = 'Admin1', c.origLocation = row.origLocation,\n",
      "    c.source = 'GOBMX'\n",
      "RETURN count(c) as CASES\n",
      ";\n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02d-GOBMXCasesAdmin1.csv' AS row\n",
      "MATCH (c:Cases{id: 'COVID-19-GOBMX' + row.date + '-' + row.origLocation})\n",
      "MATCH (o:Outbreak{id: 'COVID-19'})\n",
      "MERGE (c)-[r:RELATED_TO]->(o)\n",
      "RETURN count(r) as RELATED_TO\n",
      ";\n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02d-GOBMXCasesAdmin2.csv' AS row \n",
      "MERGE (c:Cases{id: 'COVID-19-GOBMX' + row.date + '-' + row.origLocation})\n",
      "SET c.name = 'COVID-19-' + row.date, c.date = date(row.date), c.cases = toInteger(row.cumulativeCases), \n",
      "    c.deaths = toInteger(row.cumulativeDeaths), c.population = toInteger(row.population), \n",
      "    c.aggregationLevel = 'Admin2', c.origLocation = row.origLocation,\n",
      "    c.source = 'GOBMX'\n",
      "RETURN count(c) as CASES\n",
      ";\n",
      "USING PERIODIC COMMIT 1000\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///02d-GOBMXCasesAdmin2.csv' AS row\n",
      "MATCH (c:Cases{id: 'COVID-19-GOBMX' + row.date + '-' + row.origLocation})\n",
      "MATCH (o:Outbreak{id: 'COVID-19'})\n",
      "MERGE (c)-[r:RELATED_TO]->(o)\n",
      "RETURN count(r) as RELATED_TO\n",
      ";\n",
      " CASES\n",
      "7840\n",
      "RELATED_TO\n",
      "7840\n",
      "CASES\n",
      "601965\n",
      "RELATED_TO\n",
      "601965\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP02Education.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationAdmin2.csv' AS row \n",
      "MERGE (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.stateFips + '-' + row.countyFips})\n",
      "SET s.name = 'SocialCharacteristics-' + row.stateFips + '-' + row.countyFips,\n",
      "    s.stateFips = row.stateFips, \n",
      "    s.countyFips = row.countyFips,\n",
      "    s.source = row.source, \n",
      "    s.aggregationLevel = row.aggregationLevel\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationAdmin2.csv' AS row\n",
      "MATCH (a:Admin2{fips: row.countyFips, stateFips: row.stateFips})\n",
      "MATCH (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (a)-[h:HAS_SOCIAL_CHARACTERISTICS]->(s)\n",
      "RETURN count(h) as HAS_SOCIAL_CHARACTERISTICS\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationAdmin2.csv' AS row \n",
      "MERGE (e:Education{id: 'ACSDP5Y2018.DP02-' + row.stateFips + '-' + row.countyFips})\n",
      "SET e.name = 'Education-' + row.stateFips + '-' + row.countyFips,\n",
      "    e.population25YearsAndOver = toInteger(row.population25YearsAndOver),\n",
      "    e.lessThan9thGrade = toInteger(row.lessThan9thGrade),\n",
      "    e.lessThan9thGradePct = toFloat(row.lessThan9thGradePct),\n",
      "    e.grade9thTo12thNoDiploma = toInteger(row.grade9thTo12thNoDiploma),\n",
      "    e.grade9thTo12thNoDiplomaPct = toFloat(row.grade9thTo12thNoDiplomaPct),\n",
      "    e.highSchoolGraduate = toInteger(row.highSchoolGraduate),\n",
      "    e.highSchoolGraduatePct = toFloat(row.highSchoolGraduatePct),\n",
      "    e.someCollegeNoDegree = toInteger(row.someCollegeNoDegree),\n",
      "    e.someCollegeNoDegreePct = toFloat(row.someCollegeNoDegreePct),\n",
      "    e.associatesDegree = toInteger(row.associatesDegree),\n",
      "    e.associatesDegreePct = toFloat(row.associatesDegreePct),\n",
      "    e.bachelorsDegree = toInteger(row.bachelorsDegree),\n",
      "    e.bachelorsDegreePct = toFloat(row.bachelorsDegreePct),\n",
      "    e.graduateOrProfessionalDegree = toInteger(row.graduateOrProfessionalDegree),\n",
      "    e.graduateOrProfessionalDegreePct = toFloat(row.graduateOrProfessionalDegreePct),\n",
      "    e.highSchoolGraduateOrHigher = toInteger(row.highSchoolGraduateOrHigher),\n",
      "    e.highSchoolGraduateOrHigherPct = toFloat(row.highSchoolGraduateOrHigherPct),\n",
      "    e.bachelorsDegreeOrHigher = toInteger(row.bachelorsDegreeOrHigher),\n",
      "    e.bachelorsDegreeOrHigherPct = toFloat(row.bachelorsDegreeOrHigherPct),            \n",
      "    e.stateFips = row.stateFips, \n",
      "    e.countyFips = row.countyFips,\n",
      "    e.source = row.source, \n",
      "    e.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(e) as Education\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationAdmin2.csv' AS row\n",
      "MATCH (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.stateFips + '-' + row.countyFips})\n",
      "MATCH (e:Education{id: 'ACSDP5Y2018.DP02-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (s)-[h:HAS_EDUCATION]->(e)\n",
      "RETURN count(h) as HAS_EDUCATION\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationZip.csv' AS row \n",
      "MERGE (e:Education{id: 'ACSDP5Y2018.DP02-' + row.postalCode})\n",
      "SET e.name = 'Education-' + row.postalCode,\n",
      "    e.population25YearsAndOver = toInteger(row.population25YearsAndOver),\n",
      "    e.lessThan9thGrade = toInteger(row.lessThan9thGrade),\n",
      "    e.lessThan9thGradePct = toFloat(row.lessThan9thGradePct),\n",
      "    e.grade9thTo12thNoDiploma = toInteger(row.grade9thTo12thNoDiploma),\n",
      "    e.grade9thTo12thNoDiplomaPct = toFloat(row.grade9thTo12thNoDiplomaPct),\n",
      "    e.highSchoolGraduate = toInteger(row.highSchoolGraduate),\n",
      "    e.highSchoolGraduatePct = toFloat(row.highSchoolGraduatePct),\n",
      "    e.someCollegeNoDegree = toInteger(row.someCollegeNoDegree),\n",
      "    e.someCollegeNoDegreePct = toFloat(row.someCollegeNoDegreePct),\n",
      "    e.associatesDegree = toInteger(row.associatesDegree),\n",
      "    e.associatesDegreePct = toFloat(row.associatesDegreePct),\n",
      "    e.bachelorsDegree = toInteger(row.bachelorsDegree),\n",
      "    e.bachelorsDegreePct = toFloat(row.bachelorsDegreePct),\n",
      "    e.graduateOrProfessionalDegree = toInteger(row.graduateOrProfessionalDegree),\n",
      "    e.graduateOrProfessionalDegreePct = toFloat(row.graduateOrProfessionalDegreePct),\n",
      "    e.highSchoolGraduateOrHigher = toInteger(row.highSchoolGraduateOrHigher),\n",
      "    e.highSchoolGraduateOrHigherPct = toFloat(row.highSchoolGraduateOrHigherPct),\n",
      "    e.bachelorsDegreeOrHigher = toInteger(row.bachelorsDegreeOrHigher),\n",
      "    e.bachelorsDegreeOrHigherPct = toFloat(row.bachelorsDegreeOrHigherPct),  \n",
      "    e.postalCode = row.postalCode,\n",
      "    e.source = row.source, \n",
      "    e.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(e) as Education\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationZip.csv' AS row\n",
      "MERGE (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.postalCode})\n",
      "SET s.name = 'SocialCharacteristics-' + row.postalCode,\n",
      "    s.postalCode = row.postalCode,\n",
      "    s.source = row.source, \n",
      "    s.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(s) as SocialCharacteristics\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationZip.csv' AS row\n",
      "MATCH (p:PostalCode{id: 'zip' + row.postalCode})\n",
      "MATCH (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.postalCode})\n",
      "MERGE (p)-[h:HAS_SOCIAL_CHARACTERISTICS]->(s)\n",
      "RETURN count(h) as HAS_SOCIAL_CHARACTERISTICS\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationZip.csv' AS row\n",
      "MATCH (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.postalCode})\n",
      "MATCH (e:Education{id: 'ACSDP5Y2018.DP02-' + row.postalCode})\n",
      "MERGE (s)-[h:HAS_EDUCATION]->(e)\n",
      "RETURN count(h) as HAS_EDUCATION\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationTract.csv' AS row \n",
      "MERGE (e:Education{id: 'ACSDP5Y2018.DP02-' + row.tract})\n",
      "SET e.name = 'Education-' + row.tract,\n",
      "    e.population25YearsAndOver = toInteger(row.population25YearsAndOver),\n",
      "    e.lessThan9thGrade = toInteger(row.lessThan9thGrade),\n",
      "    e.lessThan9thGradePct = toFloat(row.lessThan9thGradePct),\n",
      "    e.grade9thTo12thNoDiploma = toInteger(row.grade9thTo12thNoDiploma),\n",
      "    e.grade9thTo12thNoDiplomaPct = toFloat(row.grade9thTo12thNoDiplomaPct),\n",
      "    e.highSchoolGraduate = toInteger(row.highSchoolGraduate),\n",
      "    e.highSchoolGraduatePct = toFloat(row.highSchoolGraduatePct),\n",
      "    e.someCollegeNoDegree = toInteger(row.someCollegeNoDegree),\n",
      "    e.someCollegeNoDegreePct = toFloat(row.someCollegeNoDegreePct),\n",
      "    e.associatesDegree = toInteger(row.associatesDegree),\n",
      "    e.associatesDegreePct = toFloat(row.associatesDegreePct),\n",
      "    e.bachelorsDegree = toInteger(row.bachelorsDegree),\n",
      "    e.bachelorsDegreePct = toFloat(row.bachelorsDegreePct),\n",
      "    e.graduateOrProfessionalDegree = toInteger(row.graduateOrProfessionalDegree),\n",
      "    e.graduateOrProfessionalDegreePct = toFloat(row.graduateOrProfessionalDegreePct),\n",
      "    e.highSchoolGraduateOrHigher = toInteger(row.highSchoolGraduateOrHigher),\n",
      "    e.highSchoolGraduateOrHigherPct = toFloat(row.highSchoolGraduateOrHigherPct),\n",
      "    e.bachelorsDegreeOrHigher = toInteger(row.bachelorsDegreeOrHigher),\n",
      "    e.bachelorsDegreeOrHigherPct = toFloat(row.bachelorsDegreeOrHigherPct),  \n",
      "    e.tract = row.tract,\n",
      "    e.source = row.source, \n",
      "    e.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(e) as Education\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationTract.csv' AS row\n",
      "MERGE (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.tract})\n",
      "SET s.name = 'SocialCharacteristics-' + row.tract,\n",
      "    s.tract = row.tract,\n",
      "    s.source = row.source,\n",
      "    s.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(s) as SocialCharacteristics\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationTract.csv' AS row\n",
      "MATCH (t:Tract{id: 'tract' + row.tract})\n",
      "MATCH (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.tract})\n",
      "MERGE (t)-[h:HAS_SOCIAL_CHARACTERISTICS]->(s)\n",
      "RETURN count(h) as HAS_SOCIAL_CHARACTERISTICS\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02EducationTract.csv' AS row\n",
      "MATCH (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.tract})\n",
      "MATCH (e:Education{id: 'ACSDP5Y2018.DP02-' + row.tract})\n",
      "MERGE (s)-[h:HAS_EDUCATION]->(e)\n",
      "RETURN count(h) as HAS_EDUCATION\n",
      ";HAS_SOCIAL_CHARACTERISTICS\n",
      "3142\n",
      "Education\n",
      "3142\n",
      "HAS_EDUCATION\n",
      "3142\n",
      "Education\n",
      "33120\n",
      "SocialCharacteristics\n",
      "33120\n",
      "HAS_SOCIAL_CHARACTERISTICS\n",
      "33102\n",
      "HAS_EDUCATION\n",
      "33120\n",
      "Education\n",
      "73056\n",
      "SocialCharacteristics\n",
      "73056\n",
      "HAS_SOCIAL_CHARACTERISTICS\n",
      "73056\n",
      "HAS_EDUCATION\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP02Computers.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02ComputersAdmin2.csv' AS row \n",
      "MERGE (c:Computers{id: 'ACSDP5Y2018.DP02-' + row.stateFips + '-' + row.countyFips})\n",
      "SET c.name = 'Computers-' + row.stateFips + '-' + row.countyFips,\n",
      "    c.totalHouseholds = toInteger(row.totalHouseholds),\n",
      "    c.withComputer = toInteger(row.withComputer),\n",
      "    c.withComputerPct = toFloat(row.withComputerPct),\n",
      "    c.withBroadbandInternet = toInteger(row.withBroadbandInternet),\n",
      "    c.withBroadbandInternetPct = toFloat(row.withBroadbandInternetPct),           \n",
      "    c.stateFips = row.stateFips, \n",
      "    c.countyFips = row.countyFips,\n",
      "    c.source = row.source, \n",
      "    c.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(c) as Computers\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02ComputersAdmin2.csv' AS row\n",
      "MATCH (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.stateFips + '-' + row.countyFips})\n",
      "MATCH (c:Computers{id: 'ACSDP5Y2018.DP02-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (s)-[h:HAS_COMPUTERS]->(c)\n",
      "RETURN count(h) as HAS_Computers\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02ComputersZip.csv' AS row \n",
      "MERGE (c:Computers{id: 'ACSDP5Y2018.DP02-' + row.postalCode})\n",
      "SET c.name = 'Computers-' + row.postalCode,\n",
      "    c.totalHouseholds = toInteger(row.totalHouseholds),\n",
      "    c.withComputer = toInteger(row.withComputer),\n",
      "    c.withComputerPct = toFloat(row.withComputerPct),\n",
      "    c.withBroadbandInternet = toInteger(row.withBroadbandInternet),\n",
      "    c.withBroadbandInternetPct = toFloat(row.withBroadbandInternetPct),  \n",
      "    c.postalCode = row.postalCode,\n",
      "    c.source = row.source, \n",
      "    c.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(c) as Computers\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02ComputersZip.csv' AS row\n",
      "MATCH (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.postalCode})\n",
      "MATCH (c:Computers{id: 'ACSDP5Y2018.DP02-' + row.postalCode})\n",
      "MERGE (s)-[h:HAS_COMPUTERS]->(c)\n",
      "RETURN count(h) as HAS_Computers\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02ComputersTract.csv' AS row \n",
      "MERGE (c:Computers{id: 'ACSDP5Y2018.DP02-' + row.tract})\n",
      "SET c.name = 'Computers-' + row.tract,\n",
      "    c.totalHouseholds = toInteger(row.totalHouseholds),\n",
      "    c.withComputer = toInteger(row.withComputer),\n",
      "    c.withComputerPct = toFloat(row.withComputerPct),\n",
      "    c.withBroadbandInternet = toInteger(row.withBroadbandInternet),\n",
      "    c.withBroadbandInternetPct = toFloat(row.withBroadbandInternetPct),\n",
      "    c.tract = row.tract,\n",
      "    c.source = row.source, \n",
      "    c.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(c) as Computers\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP02ComputersTract.csv' AS row\n",
      "MATCH (s:SocialCharacteristics{id: 'ACSDP5Y2018.DP02-' + row.tract})\n",
      "MATCH (c:Computers{id: 'ACSDP5Y2018.DP02-' + row.tract})\n",
      "MERGE (s)-[h:HAS_COMPUTERS]->(c)\n",
      "RETURN count(h) as HAS_Computers\n",
      ";Computers\n",
      "3142\n",
      "HAS_Computers\n",
      "3142\n",
      "Computers\n",
      "33120\n",
      "HAS_Computers\n",
      "33120\n",
      "Computers\n",
      "73056\n",
      "HAS_Computers\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP03Employment.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentAdmin2.csv' AS row \n",
      "MERGE (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "SET e.name = 'Economics-' + row.stateFips + '-' + row.countyFips\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentAdmin2.csv' AS row\n",
      "MATCH (a:Admin2{fips: row.countyFips, stateFips: row.stateFips})\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (a)-[h:HAS_ECONOMICS]->(e)\n",
      "RETURN count(h) as HAS_ECONOMICS\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentAdmin2.csv' AS row \n",
      "MERGE (e:Employment{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "SET e.name = 'Employment-' + row.stateFips + '-' + row.countyFips,\n",
      "    e.population16YearsAndOver = toInteger(row.population16YearsAndOver),\n",
      "    e.population16YearsAndOverInLaborForce = toInteger(row.population16YearsAndOverInLaborForce),\n",
      "    e.population16YearsAndOverInLaborForcePct = toFloat(row.population16YearsAndOverInLaborForcePct),\n",
      "    e.population16YearsAndOverInCivilianLaborForce = toInteger(row.population16YearsAndOverInCivilianLaborForce),\n",
      "    e.population16YearsAndOverInCivilianLaborForcePct = toFloat(row.population16YearsAndOverInCivilianLaborForcePct),\n",
      "    e.population16YearsAndOverInArmedForces = toInteger(row.population16YearsAndOverInArmedForces),\n",
      "    e.population16YearsAndOverInArmedForcesPct = toFloat(row.population16YearsAndOverInArmedForcesPct),\n",
      "    e.population16YearsAndOverNotInLaborForce = toInteger(row.population16YearsAndOverNotInLaborForce),\n",
      "    e.population16YearsAndOverNotInLaborForcePct = toInteger(row.population16YearsAndOverNotInLaborForcePct),                           e.stateFips = row.stateFips, e.countyFips = row.countyFips,\n",
      "    e.source = row.source, e.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(e) as Employment\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentAdmin2.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MATCH (es:Employment{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (e)-[h:HAS_EMPLOYMENT]->(es)\n",
      "RETURN count(h) as HAS_EMPLOYMENT\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentZip.csv' AS row \n",
      "MERGE (e:Employment{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "SET e.name = 'Employment-' + row.postalCode,\n",
      "    e.population16YearsAndOver = toInteger(row.population16YearsAndOver),\n",
      "    e.population16YearsAndOverInLaborForce = toInteger(row.population16YearsAndOverInLaborForce),\n",
      "    e.population16YearsAndOverInLaborForcePct = toFloat(row.population16YearsAndOverInLaborForcePct),\n",
      "    e.population16YearsAndOverInCivilianLaborForce = toInteger(row.population16YearsAndOverInCivilianLaborForce),\n",
      "    e.population16YearsAndOverInCivilianLaborForcePct = toFloat(row.population16YearsAndOverInCivilianLaborForcePct),\n",
      "    e.population16YearsAndOverInArmedForces = toInteger(row.population16YearsAndOverInArmedForces),\n",
      "    e.population16YearsAndOverInArmedForcesPct = toFloat(row.population16YearsAndOverInArmedForcesPct),\n",
      "    e.population16YearsAndOverNotInLaborForce = toInteger(row.population16YearsAndOverNotInLaborForce),\n",
      "    e.population16YearsAndOverNotInLaborForcePct = toInteger(row.population16YearsAndOverNotInLaborForcePct),\n",
      "    e.postalCode = row.postalCode,\n",
      "    e.source = row.source, e.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(e) as Employment\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentZip.csv' AS row\n",
      "MERGE (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "SET e.name = 'Economics-' + row.postalCode\n",
      "RETURN count(e) as Economics\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentZip.csv' AS row\n",
      "MATCH (p:PostalCode{id: 'zip' + row.postalCode})\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MERGE (p)-[h:HAS_ECONOMICS]->(e)\n",
      "RETURN count(h) as HAS_ECONOMICS\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentZip.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MATCH (es:Employment{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MERGE (e)-[h:HAS_EMPLOYMENT]->(es)\n",
      "RETURN count(h) as HAS_EMPLOYMENT\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentTract.csv' AS row \n",
      "MERGE (e:Employment{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "SET e.name = 'Employment-' + row.tract,\n",
      "    e.population16YearsAndOver = toInteger(row.population16YearsAndOver),\n",
      "    e.population16YearsAndOverInLaborForce = toInteger(row.population16YearsAndOverInLaborForce),\n",
      "    e.population16YearsAndOverInLaborForcePct = toFloat(row.population16YearsAndOverInLaborForcePct),\n",
      "    e.population16YearsAndOverInCivilianLaborForce = toInteger(row.population16YearsAndOverInCivilianLaborForce),\n",
      "    e.population16YearsAndOverInCivilianLaborForcePct = toFloat(row.population16YearsAndOverInCivilianLaborForcePct),\n",
      "    e.population16YearsAndOverInArmedForces = toInteger(row.population16YearsAndOverInArmedForces),\n",
      "    e.population16YearsAndOverInArmedForcesPct = toFloat(row.population16YearsAndOverInArmedForcesPct),\n",
      "    e.population16YearsAndOverNotInLaborForce = toInteger(row.population16YearsAndOverNotInLaborForce),\n",
      "    e.population16YearsAndOverNotInLaborForcePct = toInteger(row.population16YearsAndOverNotInLaborForcePct),\n",
      "    e.tract = row.tract,\n",
      "    e.source = row.source, e.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(e) as Employment\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentTract.csv' AS row\n",
      "MERGE (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "SET e.name = 'Economics-' + row.tract\n",
      "RETURN count(e) as Economics\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentTract.csv' AS row\n",
      "MATCH (t:Tract{id: 'tract' + row.tract})\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MERGE (t)-[h:HAS_ECONOMICS]->(e)\n",
      "RETURN count(h) as HAS_ECONOMICS\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03EmploymentTract.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MATCH (es:Employment{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MERGE (e)-[h:HAS_EMPLOYMENT]->(es)\n",
      "RETURN count(h) as HAS_EMPLOYMENT\n",
      ";HAS_ECONOMICS\n",
      "3142\n",
      "Employment\n",
      "3142\n",
      "HAS_EMPLOYMENT\n",
      "3142\n",
      "Employment\n",
      "33120\n",
      "Economics\n",
      "33120\n",
      "HAS_ECONOMICS\n",
      "33102\n",
      "HAS_EMPLOYMENT\n",
      "33120\n",
      "Employment\n",
      "73056\n",
      "Economics\n",
      "73056\n",
      "HAS_ECONOMICS\n",
      "73056\n",
      "HAS_EMPLOYMENT\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP03Income.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03IncomeAdmin2.csv' AS row \n",
      "MERGE (i:Income{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "SET i.name = 'Income-' + row.stateFips + '-' + row.countyFips,\n",
      "    i.totalHouseholds = toInteger(row.totalHouseholds),\n",
      "    i.householdIncomeLessThan10000USD = toInteger(row.householdIncomeLessThan10000USD),\n",
      "    i.householdIncomeLessThan10000USDPct = toFloat(row.householdIncomeLessThan10000USDPct),\n",
      "    i.householdIncome10000To14999USD = toInteger(row.householdIncome10000To14999USD),\n",
      "    i.householdIncome10000To14999USDPct = toFloat(row.householdIncome10000To14999USDPct),\n",
      "    i.householdIncome15000To24999USD = toInteger(row.householdIncome15000To24999USD),\n",
      "    i.householdIncome15000To24999USDPct = toFloat(row.householdIncome15000To24999USDPct),\n",
      "    i.householdIncome25000To34999USD = toInteger(row.householdIncome25000To34999USD),\n",
      "    i.householdIncome25000To34999USDPct = toFloat(row.householdIncome25000To34999USDPct),\n",
      "    i.householdIncome35000To49999USD = toInteger(row.householdIncome35000To49999USD),\n",
      "    i.householdIncome35000To49999USDPct = toFloat(row.householdIncome35000To49999USDPct),\n",
      "    i.householdIncome50000To74999USD = toInteger(row.householdIncome50000To74999USD),\n",
      "    i.householdIncome50000To74999USDPct = toFloat(row.householdIncome50000To74999USDPct),\n",
      "    i.householdIncome75000To99999USD = toInteger(row.householdIncome75000To99999USD),\n",
      "    i.householdIncome75000To99999USDPct = toFloat(row.householdIncome75000To99999USDPct),\n",
      "    i.householdIncome100000To149999USD = toInteger(row.householdIncome100000To149999USD),\n",
      "    i.householdIncome100000To149999USDPct = toFloat(row.householdIncome100000To149999USDPct),\n",
      "    i.householdIncome150000To199999USD = toInteger(row.householdIncome150000To199999USD),\n",
      "    i.householdIncome150000To199999USDPct = toFloat(row.householdIncome150000To199999USDPct),\n",
      "    i.householdIncomeMoreThan200000USD = toInteger(row.householdIncomeMoreThan200000USD),\n",
      "    i.householdIncomeMoreThan200000USDPct = toFloat(row.householdIncomeMoreThan200000USDPct),\n",
      "    i.medianHouseholdIncomeUSD = toInteger(row.medianHouseholdIncomeUSD),\n",
      "    i.meanHouseholdIncomeUSD = toInteger(row.meanHouseholdIncomeUSD),\n",
      "    i.stateFips = row.stateFips, i.countyFips = row.countyFips,\n",
      "    i.source = row.source, i.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(i) as Income\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03IncomeAdmin2.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MATCH (i:Income{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (e)-[h:HAS_INCOME]->(i)\n",
      "RETURN count(h) as HAS_INCOME\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03IncomeZip.csv' AS row \n",
      "MERGE (i:Income{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "SET i.name = 'Income-' + row.postalCode,\n",
      "    i.totalHouseholds = toInteger(row.totalHouseholds),\n",
      "    i.householdIncomeLessThan10000USD = toInteger(row.householdIncomeLessThan10000USD),\n",
      "    i.householdIncomeLessThan10000USDPct = toFloat(row.householdIncomeLessThan10000USDPct),\n",
      "    i.householdIncome10000To14999USD = toInteger(row.householdIncome10000To14999USD),\n",
      "    i.householdIncome10000To14999USDPct = toFloat(row.householdIncome10000To14999USDPct),\n",
      "    i.householdIncome15000To24999USD = toInteger(row.householdIncome15000To24999USD),\n",
      "    i.householdIncome15000To24999USDPct = toFloat(row.householdIncome15000To24999USDPct),\n",
      "    i.householdIncome25000To34999USD = toInteger(row.householdIncome25000To34999USD),\n",
      "    i.householdIncome25000To34999USDPct = toFloat(row.householdIncome25000To34999USDPct),\n",
      "    i.householdIncome35000To49999USD = toInteger(row.householdIncome35000To49999USD),\n",
      "    i.householdIncome35000To49999USDPct = toFloat(row.householdIncome35000To49999USDPct),\n",
      "    i.householdIncome50000To74999USD = toInteger(row.householdIncome50000To74999USD),\n",
      "    i.householdIncome50000To74999USDPct = toFloat(row.householdIncome50000To74999USDPct),\n",
      "    i.householdIncome75000To99999USD = toInteger(row.householdIncome75000To99999USD),\n",
      "    i.householdIncome75000To99999USDPct = toFloat(row.householdIncome75000To99999USDPct),\n",
      "    i.householdIncome100000To149999USD = toInteger(row.householdIncome100000To149999USD),\n",
      "    i.householdIncome100000To149999USDPct = toFloat(row.householdIncome100000To149999USDPct),\n",
      "    i.householdIncome150000To199999USD = toInteger(row.householdIncome150000To199999USD),\n",
      "    i.householdIncome150000To199999USDPct = toFloat(row.householdIncome150000To199999USDPct),\n",
      "    i.householdIncomeMoreThan200000USD = toInteger(row.householdIncomeMoreThan200000USD),\n",
      "    i.householdIncomeMoreThan200000USDPct = toFloat(row.householdIncomeMoreThan200000USDPct),\n",
      "    i.medianHouseholdIncomeUSD = toInteger(row.medianHouseholdIncomeUSD),\n",
      "    i.meanHouseholdIncomeUSD = toInteger(row.meanHouseholdIncomeUSD),\n",
      "    i.postalCode = row.postalCode,\n",
      "    i.source = row.source, i.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(i) as Income\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03IncomeZip.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MATCH (i:Income{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MERGE (e)-[h:HAS_INCOME]->(i)\n",
      "RETURN count(h) as HAS_INCOME\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03IncomeTract.csv' AS row \n",
      "MERGE (i:Income{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "SET i.name = 'Income-' + row.tract,\n",
      "    i.totalHouseholds = toInteger(row.totalHouseholds),\n",
      "    i.householdIncomeLessThan10000USD = toInteger(row.householdIncomeLessThan10000USD),\n",
      "    i.householdIncomeLessThan10000USDPct = toFloat(row.householdIncomeLessThan10000USDPct),\n",
      "    i.householdIncome10000To14999USD = toInteger(row.householdIncome10000To14999USD),\n",
      "    i.householdIncome10000To14999USDPct = toFloat(row.householdIncome10000To14999USDPct),\n",
      "    i.householdIncome15000To24999USD = toInteger(row.householdIncome15000To24999USD),\n",
      "    i.householdIncome15000To24999USDPct = toFloat(row.householdIncome15000To24999USDPct),\n",
      "    i.householdIncome25000To34999USD = toInteger(row.householdIncome25000To34999USD),\n",
      "    i.householdIncome25000To34999USDPct = toFloat(row.householdIncome25000To34999USDPct),\n",
      "    i.householdIncome35000To49999USD = toInteger(row.householdIncome35000To49999USD),\n",
      "    i.householdIncome35000To49999USDPct = toFloat(row.householdIncome35000To49999USDPct),\n",
      "    i.householdIncome50000To74999USD = toInteger(row.householdIncome50000To74999USD),\n",
      "    i.householdIncome50000To74999USDPct = toFloat(row.householdIncome50000To74999USDPct),\n",
      "    i.householdIncome75000To99999USD = toInteger(row.householdIncome75000To99999USD),\n",
      "    i.householdIncome75000To99999USDPct = toFloat(row.householdIncome75000To99999USDPct),\n",
      "    i.householdIncome100000To149999USD = toInteger(row.householdIncome100000To149999USD),\n",
      "    i.householdIncome100000To149999USDPct = toFloat(row.householdIncome100000To149999USDPct),\n",
      "    i.householdIncome150000To199999USD = toInteger(row.householdIncome150000To199999USD),\n",
      "    i.householdIncome150000To199999USDPct = toFloat(row.householdIncome150000To199999USDPct),\n",
      "    i.householdIncomeMoreThan200000USD = toInteger(row.householdIncomeMoreThan200000USD),\n",
      "    i.householdIncomeMoreThan200000USDPct = toFloat(row.householdIncomeMoreThan200000USDPct),\n",
      "    i.medianHouseholdIncomeUSD = toInteger(row.medianHouseholdIncomeUSD),\n",
      "    i.meanHouseholdIncomeUSD = toInteger(row.meanHouseholdIncomeUSD),\n",
      "    i.tract = row.tract,\n",
      "    i.source = row.source, i.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(i) as Income\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03IncomeTract.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MATCH (i:Income{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MERGE (e)-[h:HAS_INCOME]->(i)\n",
      "RETURN count(h) as HAS_INCOME\n",
      ";Income\n",
      "3142\n",
      "HAS_INCOME\n",
      "3142\n",
      "Income\n",
      "33120\n",
      "HAS_INCOME\n",
      "33120\n",
      "Income\n",
      "73056\n",
      "HAS_INCOME\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP03HealthInsurance.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03HealthInsuranceAdmin2.csv' AS row \n",
      "MERGE (h:HealthInsurance{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "SET h.name = 'HealthInsurance-' + row.stateFips + '-' + row.countyFips,\n",
      "    h.civilianNoninstitutionalizedPopulation = toInteger(row.civilianNoninstitutionalizedPopulation),\n",
      "    h.withHealthInsuranceCoverage = toInteger(row.withHealthInsuranceCoverage),\n",
      "    h.withHealthInsuranceCoveragePct = toFloat(row.withHealthInsuranceCoveragePct),\n",
      "    h.withPrivateHealthInsurance = toInteger(row.withPrivateHealthInsurance),\n",
      "    h.withPrivateHealthInsurancePct = toFloat(row.withPrivateHealthInsurancePct),\n",
      "    h.withPublicCoverage = toInteger(row.withPublicCoverage),\n",
      "    h.withPublicCoveragePct = toFloat(row.withPublicCoveragePct),\n",
      "    h.noHealthInsuranceCoverage = toInteger(row.noHealthInsuranceCoverage),\n",
      "    h.noHealthInsuranceCoveragePct = toFloat(row.noHealthInsuranceCoveragePct),\n",
      "    h.stateFips = row.stateFips, h.countyFips = row.countyFips,\n",
      "    h.source = row.source, h.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(h) as HealthInsurance\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03HealthInsuranceAdmin2.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MATCH (h:HealthInsurance{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (e)-[hh:HAS_HEALTH_INSURANCE]->(h)\n",
      "RETURN count(hh) as HAS_HEALTH_INSURANCE\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03HealthInsuranceZip.csv' AS row \n",
      "MERGE (h:HealthInsurance{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "SET h.name = 'HealthInsurance-' + row.postalCode,\n",
      "    h.civilianNoninstitutionalizedPopulation = toInteger(row.civilianNoninstitutionalizedPopulation),\n",
      "    h.withHealthInsuranceCoverage = toInteger(row.withHealthInsuranceCoverage),\n",
      "    h.withHealthInsuranceCoveragePct = toFloat(row.withHealthInsuranceCoveragePct),\n",
      "    h.withPrivateHealthInsurance = toInteger(row.withPrivateHealthInsurance),\n",
      "    h.withPrivateHealthInsurancePct = toFloat(row.withPrivateHealthInsurancePct),\n",
      "    h.withPublicCoverage = toInteger(row.withPublicCoverage),\n",
      "    h.withPublicCoveragePct = toFloat(row.withPublicCoveragePct),\n",
      "    h.noHealthInsuranceCoverage = toInteger(row.noHealthInsuranceCoverage),\n",
      "    h.noHealthInsuranceCoveragePct = toFloat(row.noHealthInsuranceCoveragePct),\n",
      "    h.postalCode = row.postalCode,\n",
      "    h.source = row.source, h.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(h) as HealthInsurance\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03HealthInsuranceZip.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MATCH (h:HealthInsurance{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MERGE (e)-[hh:HAS_HEALTH_INSURANCE]->(h)\n",
      "RETURN count(hh) as HAS_HEALTH_INSURANCE\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03HealthInsuranceTract.csv' AS row \n",
      "MERGE (h:HealthInsurance{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "SET h.name = 'HealthInsurance-' + row.tract,\n",
      "    h.civilianNoninstitutionalizedPopulation = toInteger(row.civilianNoninstitutionalizedPopulation),\n",
      "    h.withHealthInsuranceCoverage = toInteger(row.withHealthInsuranceCoverage),\n",
      "    h.withHealthInsuranceCoveragePct = toFloat(row.withHealthInsuranceCoveragePct),\n",
      "    h.withPrivateHealthInsurance = toInteger(row.withPrivateHealthInsurance),\n",
      "    h.withPrivateHealthInsurancePct = toFloat(row.withPrivateHealthInsurancePct),\n",
      "    h.withPublicCoverage = toInteger(row.withPublicCoverage),\n",
      "    h.withPublicCoveragePct = toFloat(row.withPublicCoveragePct),\n",
      "    h.noHealthInsuranceCoverage = toInteger(row.noHealthInsuranceCoverage),\n",
      "    h.noHealthInsuranceCoveragePct = toFloat(row.noHealthInsuranceCoveragePct),\n",
      "    h.tract = row.tract,\n",
      "    h.source = row.source, h.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(h) as HealthInsurance\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03HealthInsuranceTract.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MATCH (h:HealthInsurance{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MERGE (e)-[hh:HAS_HEALTH_INSURANCE]->(i)\n",
      "RETURN count(hh) as HAS_HEALTH_INSURANCE\n",
      ";HealthInsurance\n",
      "3142\n",
      "HAS_HEALTH_INSURANCE\n",
      "3142\n",
      "HealthInsurance\n",
      "33120\n",
      "HAS_HEALTH_INSURANCE\n",
      "33120\n",
      "HealthInsurance\n",
      "73056\n",
      "HAS_HEALTH_INSURANCE\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP03Commuting.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03CommutingAdmin2.csv' AS row \n",
      "MERGE (c:Commuting{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "SET c.name = 'Commuting-' + row.stateFips + '-' + row.countyFips,\n",
      "    c.workers16YearsAndOver = toInteger(row.workers16YearsAndOver),\n",
      "    c.droveAloneToWorkInCarTruckOrVan = toInteger(row.droveAloneToWorkInCarTruckOrVan),\n",
      "    c.droveAloneToWorkInCarTruckOrVanPct = toFloat(row.droveAloneToWorkInCarTruckOrVanPct),\n",
      "    c.carpooledToWorkInCarTruckOrVan = toInteger(row.carpooledToWorkInCarTruckOrVan),\n",
      "    c.carpooledToWorkInCarTruckOrVanPct = toFloat(row.carpooledToWorkInCarTruckOrVanPct),\n",
      "    c.publicTransportToWork = toInteger(row.publicTransportToWork),\n",
      "    c.publicTransportToWorkPct = toFloat(row.publicTransportToWorkPct),\n",
      "    c.walkedToWork = toInteger(row.walkedToWork),\n",
      "    c.walkedToWorkPct = toFloat(row.walkedToWorkPct),\n",
      "    c.otherMeansOfCommutingToWork = toInteger(row.otherMeansOfCommutingToWork),\n",
      "    c.otherMeansOfCommutingToWorkPct = toFloat(row.otherMeansOfCommutingToWorkPct),\n",
      "    c.workedAtHome = toInteger(row.workedAtHome),\n",
      "    c.workedAtHomePct = toFloat(row.workedAtHomePct),\n",
      "    c.meanTravelTimeToWorkMinutes = toInteger(row.meanTravelTimeToWorkMinutes),\n",
      "    c.stateFips = row.stateFips, c.countyFips = row.countyFips,\n",
      "    c.source = row.source, c.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(c) as Commuting\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03CommutingAdmin2.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MATCH (c:Commuting{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (e)-[h:HAS_COMMUTING]->(c)\n",
      "RETURN count(h) as HAS_COMMUTING\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03CommutingZip.csv' AS row \n",
      "MERGE (c:Commuting{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "SET c.name = 'Commuting-' + row.postalCode,\n",
      "    c.workers16YearsAndOver = toInteger(row.workers16YearsAndOver),\n",
      "    c.droveAloneToWorkInCarTruckOrVan = toInteger(row.droveAloneToWorkInCarTruckOrVan),\n",
      "    c.droveAloneToWorkInCarTruckOrVanPct = toFloat(row.droveAloneToWorkInCarTruckOrVanPct),\n",
      "    c.carpooledToWorkInCarTruckOrVan = toInteger(row.carpooledToWorkInCarTruckOrVan),\n",
      "    c.carpooledToWorkInCarTruckOrVanPct = toFloat(row.carpooledToWorkInCarTruckOrVanPct),\n",
      "    c.publicTransportToWork = toInteger(row.publicTransportToWork),\n",
      "    c.publicTransportToWorkPct = toFloat(row.publicTransportToWorkPct),\n",
      "    c.walkedToWork = toInteger(row.walkedToWork),\n",
      "    c.walkedToWorkPct = toFloat(row.walkedToWorkPct),\n",
      "    c.otherMeansOfCommutingToWork = toInteger(row.otherMeansOfCommutingToWork),\n",
      "    c.otherMeansOfCommutingToWorkPct = toFloat(row.otherMeansOfCommutingToWorkPct),\n",
      "    c.workedAtHome = toInteger(row.workedAtHome),\n",
      "    c.workedAtHomePct = toFloat(row.workedAtHomePct),\n",
      "    c.meanTravelTimeToWorkMinutes = toInteger(row.meanTravelTimeToWorkMinutes),\n",
      "    c.postalCode = row.postalCode,\n",
      "    c.source = row.source, c.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(c) as Commuting\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03CommutingZip.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MATCH (c:Commuting{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MERGE (e)-[h:HAS_COMMUTING]->(c)\n",
      "RETURN count(h) as HAS_COMMUTING\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03CommutingTract.csv' AS row \n",
      "MERGE (c:Commuting{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "SET c.name = 'Commuting-' + row.tract,\n",
      "    c.workers16YearsAndOver = toInteger(row.workers16YearsAndOver),\n",
      "    c.droveAloneToWorkInCarTruckOrVan = toInteger(row.droveAloneToWorkInCarTruckOrVan),\n",
      "    c.droveAloneToWorkInCarTruckOrVanPct = toFloat(row.droveAloneToWorkInCarTruckOrVanPct),\n",
      "    c.carpooledToWorkInCarTruckOrVan = toInteger(row.carpooledToWorkInCarTruckOrVan),\n",
      "    c.carpooledToWorkInCarTruckOrVanPct = toFloat(row.carpooledToWorkInCarTruckOrVanPct),\n",
      "    c.publicTransportToWork = toInteger(row.publicTransportToWork),\n",
      "    c.publicTransportToWorkPct = toFloat(row.publicTransportToWorkPct),\n",
      "    c.walkedToWork = toInteger(row.walkedToWork),\n",
      "    c.walkedToWorkPct = toFloat(row.walkedToWorkPct),\n",
      "    c.otherMeansOfCommutingToWork = toInteger(row.otherMeansOfCommutingToWork),\n",
      "    c.otherMeansOfCommutingToWorkPct = toFloat(row.otherMeansOfCommutingToWorkPct),\n",
      "    c.workedAtHome = toInteger(row.workedAtHome),\n",
      "    c.workedAtHomePct = toFloat(row.workedAtHomePct),\n",
      "    c.meanTravelTimeToWorkMinutes = toInteger(row.meanTravelTimeToWorkMinutes),\n",
      "    c.tract = row.tract,\n",
      "    c.source = row.source, c.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(c) as Commuting\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03CommutingTract.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MATCH (c:Commuting{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MERGE (e)-[h:HAS_COMMUTING]->(c)\n",
      "RETURN count(h) as HAS_COMMUTING\n",
      ";Commuting\n",
      "3142\n",
      "HAS_COMMUTING\n",
      "3142\n",
      "Commuting\n",
      "33120\n",
      "HAS_COMMUTING\n",
      "33120\n",
      "Commuting\n",
      "73056\n",
      "HAS_COMMUTING\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP03Occupation.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03OccupationAdmin2.csv' AS row \n",
      "MERGE (o:Occupation{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "SET o.name = 'Occupation-' + row.stateFips + '-' + row.countyFips,\n",
      "    o.civilianEmployedPopulation16YearsAndOver = toInteger(row.civilianEmployedPopulation16YearsAndOver),\n",
      "    o.managementBusinessScienceAndArtsOccupations = toInteger(row.managementBusinessScienceAndArtsOccupations),\n",
      "    o.managementBusinessScienceAndArtsOccupationsPct = toFloat(row.managementBusinessScienceAndArtsOccupationsPct),\n",
      "    o.serviceOccupations = toInteger(row.serviceOccupations),\n",
      "    o.serviceOccupationsPct = toFloat(row.serviceOccupationsPct),\n",
      "    o.salesAndOfficeOccupations = toInteger(row.salesAndOfficeOccupations),\n",
      "    o.salesAndOfficeOccupationsPct = toFloat(row.salesAndOfficeOccupationsPct),\n",
      "    o.naturalResourcesConstructionAndMaintenanceOccupations = toInteger(row.naturalResourcesConstructionAndMaintenanceOccupations),\n",
      "    o.naturalResourcesConstructionAndMaintenanceOccupationsPct = toFloat(row.naturalResourcesConstructionAndMaintenanceOccupationsPct),\n",
      "    o.productionTransportationAndMaterialMovingOccupations = toInteger(row.productionTransportationAndMaterialMovingOccupations),\n",
      "    o.productionTransportationAndMaterialMovingOccupationsPct = toFloat(row.productionTransportationAndMaterialMovingOccupationsPct),\n",
      "    o.stateFips = row.stateFips, o.countyFips = row.countyFips,\n",
      "    o.source = row.source, o.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(o) as Occupation\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03OccupationAdmin2.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MATCH (o:Occupation{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (e)-[h:HAS_OCCUPATION]->(o)\n",
      "RETURN count(h) as HAS_OCCUPATION\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03OccupationZip.csv' AS row \n",
      "MERGE (o:Occupation{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "SET o.name = 'Occupation-' + row.postalCode,\n",
      "    o.civilianEmployedPopulation16YearsAndOver = toInteger(row.civilianEmployedPopulation16YearsAndOver),\n",
      "    o.managementBusinessScienceAndArtsOccupations = toInteger(row.managementBusinessScienceAndArtsOccupations),\n",
      "    o.managementBusinessScienceAndArtsOccupationsPct = toFloat(row.managementBusinessScienceAndArtsOccupationsPct),\n",
      "    o.serviceOccupations = toInteger(row.serviceOccupations),\n",
      "    o.serviceOccupationsPct = toFloat(row.serviceOccupationsPct),\n",
      "    o.salesAndOfficeOccupations = toInteger(row.salesAndOfficeOccupations),\n",
      "    o.salesAndOfficeOccupationsPct = toFloat(row.salesAndOfficeOccupationsPct),\n",
      "    o.naturalResourcesConstructionAndMaintenanceOccupations = toInteger(row.naturalResourcesConstructionAndMaintenanceOccupations),\n",
      "    o.naturalResourcesConstructionAndMaintenanceOccupationsPct = toFloat(row.naturalResourcesConstructionAndMaintenanceOccupationsPct),\n",
      "    o.productionTransportationAndMaterialMovingOccupations = toInteger(row.productionTransportationAndMaterialMovingOccupations),\n",
      "    o.productionTransportationAndMaterialMovingOccupationsPct = toFloat(row.productionTransportationAndMaterialMovingOccupationsPct),       o.postalCode = row.postalCode,\n",
      "    o.source = row.source, o.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(o) as Occupation\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03OccupationZip.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MATCH (o:Occupation{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MERGE (e)-[h:HAS_OCCUPATION]->(o)\n",
      "RETURN count(h) as HAS_OCCUPATION\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03OccupationTract.csv' AS row \n",
      "MERGE (o:Occupation{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "SET o.name = 'Occupation-' + row.tract,\n",
      "    o.civilianEmployedPopulation16YearsAndOver = toInteger(row.civilianEmployedPopulation16YearsAndOver),\n",
      "    o.managementBusinessScienceAndArtsOccupations = toInteger(row.managementBusinessScienceAndArtsOccupations),\n",
      "    o.managementBusinessScienceAndArtsOccupationsPct = toFloat(row.managementBusinessScienceAndArtsOccupationsPct),\n",
      "    o.serviceOccupations = toInteger(row.serviceOccupations),\n",
      "    o.serviceOccupationsPct = toFloat(row.serviceOccupationsPct),\n",
      "    o.salesAndOfficeOccupations = toInteger(row.salesAndOfficeOccupations),\n",
      "    o.salesAndOfficeOccupationsPct = toFloat(row.salesAndOfficeOccupationsPct),\n",
      "    o.naturalResourcesConstructionAndMaintenanceOccupations = toInteger(row.naturalResourcesConstructionAndMaintenanceOccupations),\n",
      "    o.naturalResourcesConstructionAndMaintenanceOccupationsPct = toFloat(row.naturalResourcesConstructionAndMaintenanceOccupationsPct),\n",
      "    o.productionTransportationAndMaterialMovingOccupations = toInteger(row.productionTransportationAndMaterialMovingOccupations),\n",
      "    o.productionTransportationAndMaterialMovingOccupationsPct = toFloat(row.productionTransportationAndMaterialMovingOccupationsPct),\n",
      "    o.tract = row.tract,\n",
      "    o.source = row.source, o.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(o) as Occupation\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03OccupationTract.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MATCH (o:Occupation{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MERGE (e)-[h:HAS_OCCUPATION]->(o)\n",
      "RETURN count(h) as HAS_OCCUPATION\n",
      ";Occupation\n",
      "3142\n",
      "HAS_OCCUPATION\n",
      "3142\n",
      "Occupation\n",
      "33120\n",
      "HAS_OCCUPATION\n",
      "33120\n",
      "Occupation\n",
      "73056\n",
      "HAS_OCCUPATION\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP03Poverty.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03PovertyAdmin2.csv' AS row \n",
      "MERGE (p:Poverty{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "SET p.name = 'Poverty-' + row.stateFips + '-' + row.countyFips,\n",
      "    p.povertyAllFamiliesPct = toFloat(row.povertyAllFamiliesPct),\n",
      "    p.povertyAllPeoplePct = toFloat(row.povertyAllPeoplePct),\n",
      "    p.stateFips = row.stateFips,\n",
      "    p.countyFips = row.countyFips,\n",
      "    p.source = row.source, \n",
      "    p.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(p) as Poverty\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03PovertyAdmin2.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MATCH (p:Poverty{id: 'ACSDP5Y2018.DP03-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (e)-[h:HAS_POVERTY]->(p)\n",
      "RETURN count(h) as HAS_POVERTY\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03PovertyZip.csv' AS row \n",
      "MERGE (p:Poverty{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "SET p.name = 'Poverty-' + row.postalCode,       \n",
      "    p.povertyAllFamiliesPct = toFloat(row.povertyAllFamiliesPct),\n",
      "    p.povertyAllPeoplePct = toFloat(row.povertyAllPeoplePct),\n",
      "    p.postalCode = row.postalCode,\n",
      "    p.source = row.source, \n",
      "    p.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(p) as Poverty\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03PovertyZip.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MATCH (p:Poverty{id: 'ACSDP5Y2018.DP03-' + row.postalCode})\n",
      "MERGE (e)-[h:HAS_POVERTY]->(p)\n",
      "RETURN count(h) as HAS_POVERTY\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03PovertyTract.csv' AS row \n",
      "MERGE (p:Poverty{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "SET p.name = 'Poverty-' + row.tract,\n",
      "    p.povertyAllFamiliesPct = toFloat(row.povertyAllFamiliesPct),\n",
      "    p.povertyAllPeoplePct = toFloat(row.povertyAllPeoplePct),\n",
      "    p.tract = row.tract,\n",
      "    p.source = row.source, \n",
      "    p.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(p) as Poverty\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP03PovertyTract.csv' AS row\n",
      "MATCH (e:Economics{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MATCH (p:Poverty{id: 'ACSDP5Y2018.DP03-' + row.tract})\n",
      "MERGE (e)-[h:HAS_POVERTY]->(p)\n",
      "RETURN count(h) as HAS_POVERTY\n",
      ";Poverty\n",
      "3142\n",
      "HAS_POVERTY\n",
      "3142\n",
      "Poverty\n",
      "33120\n",
      "HAS_POVERTY\n",
      "33120\n",
      "Poverty\n",
      "73056\n",
      "HAS_POVERTY\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP04.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP04Admin2.csv' AS row \n",
      "MERGE (h:Housing{id: 'ACSDP5Y2018.DP04-' + row.stateFips + '-' + row.countyFips})\n",
      "SET h.name = 'Housing-' + row.stateFips + '-' + row.countyFips,\n",
      "    h.medianRoomsInHousingUnits = toFloat(row.medianRoomsInHousingUnits),\n",
      "    h.ownerOccupiedHousingUnits = toInteger(row.ownerOccupiedHousingUnits),\n",
      "    h.ownerOccupiedHousingUnitsPct = toFloat(row.ownerOccupiedHousingUnitsPct),\n",
      "    h.renterOccupiedHousingUnits = toInteger(row.renterOccupiedHousingUnits),\n",
      "    h.renterOccupiedHousingUnitsPct = toFloat(row.renterOccupiedHousingUnitsPct),\n",
      "    h.averageHouseholdSizeOfOwnerOccupiedUnit = toFloat(row.averageHouseholdSizeOfOwnerOccupiedUnit),\n",
      "    h.averageHouseholdSizeOfRenterOccupiedUnit = toFloat(row.averageHouseholdSizeOfRenterOccupiedUnit),\n",
      "    h.occupiedHousingUnitsWithVehicles = toInteger(row.occupiedHousingUnitsWithVehicles),\n",
      "    h.occupiedHousingUnitsNoVehicles = toInteger(row.occupiedHousingUnitsNoVehicles),\n",
      "    h.occupiedHousingUnitsNoVehiclesPct = toFloat(row.occupiedHousingUnitsNoVehiclesPct),\n",
      "    h.`occupantsPerRoom1.00orLess` = toInteger(row.`occupantsPerRoom1.00orLess`),\n",
      "    h.`occupantsPerRoom1.00orLessPct` = toFloat(row.`occupantsPerRoom1.00orLessPct`),\n",
      "    h.`occupantsPerRoom1.01to1.50` = toInteger(row.`occupantsPerRoom1.01to1.50`),\n",
      "    h.`occupantsPerRoom1.01to1.50Pct` = toFloat(row.`occupantsPerRoom1.01to1.50Pct`),\n",
      "    h.`occupantsPerRoom1.51orMore` = toInteger(row.`occupantsPerRoom1.51orMore`),\n",
      "    h.`occupantsPerRoom1.51orMorePct` = toFloat(row.`occupantsPerRoom1.51orMorePct`),\n",
      "    h.stateFips = row.stateFips, h.countyFips = row.countyFips,\n",
      "    h.source = row.source, h.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(h) as Housing\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP04Admin2.csv' AS row\n",
      "MATCH (a:Admin2{fips: row.countyFips, stateFips: row.stateFips})\n",
      "MATCH (h:Housing{id: 'ACSDP5Y2018.DP04-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (a)-[hh:HAS_HOUSING]->(h)\n",
      "RETURN count(hh) as HAS_HOUSING\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP04Zip.csv' AS row \n",
      "MERGE (h:Housing{id: 'ACSDP5Y2018.DP04-' + row.postalCode})\n",
      "SET h.name = 'Housing-' + row.postalCode,\n",
      "    h.medianRoomsInHousingUnits = toFloat(row.medianRoomsInHousingUnits),\n",
      "    h.ownerOccupiedHousingUnits = toInteger(row.ownerOccupiedHousingUnits),\n",
      "    h.ownerOccupiedHousingUnitsPct = toFloat(row.ownerOccupiedHousingUnitsPct),\n",
      "    h.renterOccupiedHousingUnits = toInteger(row.renterOccupiedHousingUnits),\n",
      "    h.renterOccupiedHousingUnitsPct = toFloat(row.renterOccupiedHousingUnitsPct),\n",
      "    h.averageHouseholdSizeOfOwnerOccupiedUnit = toFloat(row.averageHouseholdSizeOfOwnerOccupiedUnit),\n",
      "    h.averageHouseholdSizeOfRenterOccupiedUnit = toFloat(row.averageHouseholdSizeOfRenterOccupiedUnit),\n",
      "    h.occupiedHousingUnitsWithVehicles = toInteger(row.occupiedHousingUnitsWithVehicles),\n",
      "    h.occupiedHousingUnitsNoVehicles = toInteger(row.occupiedHousingUnitsNoVehicles),\n",
      "    h.occupiedHousingUnitsNoVehiclesPct = toFloat(row.occupiedHousingUnitsNoVehiclesPct),\n",
      "    h.`occupantsPerRoom1.00orLess` = toInteger(row.`occupantsPerRoom1.00orLess`),\n",
      "    h.`occupantsPerRoom1.00orLessPct` = toFloat(row.`occupantsPerRoom1.00orLessPct`),\n",
      "    h.`occupantsPerRoom1.01to1.50` = toInteger(row.`occupantsPerRoom1.01to1.50`),\n",
      "    h.`occupantsPerRoom1.01to1.50Pct` = toFloat(row.`occupantsPerRoom1.01to1.50Pct`),\n",
      "    h.`occupantsPerRoom1.51orMore` = toInteger(row.`occupantsPerRoom1.51orMore`),\n",
      "    h.`occupantsPerRoom1.51orMorePct` = toFloat(row.`occupantsPerRoom1.51orMorePct`),\n",
      "    h.postalCode = row.postalCode,\n",
      "    h.source = row.source, h.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(h) as Housing\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP04Zip.csv' AS row\n",
      "MATCH (p:PostalCode{id: 'zip' + row.postalCode})\n",
      "MATCH (h:Housing{id: 'ACSDP5Y2018.DP04-' + row.postalCode})\n",
      "MERGE (p)-[hh:HAS_HOUSING]->(h)\n",
      "RETURN count(hh) as HAS_HOUSING\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP04Tract.csv' AS row \n",
      "MERGE (h:Housing{id: 'ACSDP5Y2018.DP04-' + row.tract})\n",
      "SET h.name = 'Housing-' + row.tact,\n",
      "    h.medianRoomsInHousingUnits = toFloat(row.medianRoomsInHousingUnits),\n",
      "    h.ownerOccupiedHousingUnits = toInteger(row.ownerOccupiedHousingUnits),\n",
      "    h.ownerOccupiedHousingUnitsPct = toFloat(row.ownerOccupiedHousingUnitsPct),\n",
      "    h.renterOccupiedHousingUnits = toInteger(row.renterOccupiedHousingUnits),\n",
      "    h.renterOccupiedHousingUnitsPct = toFloat(row.renterOccupiedHousingUnitsPct),\n",
      "    h.averageHouseholdSizeOfOwnerOccupiedUnit = toFloat(row.averageHouseholdSizeOfOwnerOccupiedUnit),\n",
      "    h.averageHouseholdSizeOfRenterOccupiedUnit = toFloat(row.averageHouseholdSizeOfRenterOccupiedUnit),\n",
      "    h.occupiedHousingUnitsWithVehicles = toInteger(row.occupiedHousingUnitsWithVehicles),\n",
      "    h.occupiedHousingUnitsNoVehicles = toInteger(row.occupiedHousingUnitsNoVehicles),\n",
      "    h.occupiedHousingUnitsNoVehiclesPct = toFloat(row.occupiedHousingUnitsNoVehiclesPct),\n",
      "    h.`occupantsPerRoom1.00orLess` = toInteger(row.`occupantsPerRoom1.00orLess`),\n",
      "    h.`occupantsPerRoom1.00orLessPct` = toFloat(row.`occupantsPerRoom1.00orLessPct`),\n",
      "    h.`occupantsPerRoom1.01to1.50` = toInteger(row.`occupantsPerRoom1.01to1.50`),\n",
      "    h.`occupantsPerRoom1.01to1.50Pct` = toFloat(row.`occupantsPerRoom1.01to1.50Pct`),\n",
      "    h.`occupantsPerRoom1.51orMore` = toInteger(row.`occupantsPerRoom1.51orMore`),\n",
      "    h.`occupantsPerRoom1.51orMorePct` = toFloat(row.`occupantsPerRoom1.51orMorePct`),\n",
      "    h.tract = row.tract,\n",
      "    h.source = row.source, h.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(h) as Housing\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP04Tract.csv' AS row\n",
      "MATCH (t:Tract{id: 'tract' + row.tract})\n",
      "MATCH (h:Housing{id: 'ACSDP5Y2018.DP04-' + row.tract})\n",
      "MERGE (t)-[hh:HAS_HOUSING]->(h)\n",
      "RETURN count(hh) as HAS_HOUSING\n",
      ";Housing\n",
      "3142\n",
      "HAS_HOUSING\n",
      "3142\n",
      "Housing\n",
      "33120\n",
      "HAS_HOUSING\n",
      "33102\n",
      "Housing\n",
      "73056\n",
      "HAS_HOUSING\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 03a-USCensusDP05.cypher:\n",
      " \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP05Admin2.csv' AS row \n",
      "MERGE (d:Demographics{id: 'ACSDP5Y2018.DP05-' + row.stateFips + '-' + row.countyFips})\n",
      "SET d.name = 'Demographics-' + row.stateFips + '-' + row.countyFips,\n",
      "    d.totalPopulation = toInteger(row.totalPopulation), \n",
      "    d.male = toInteger(row.male), d.female = toInteger(row.female),\n",
      "    d.age0_4 = toInteger(row.age0_4), d.age5_9 = toInteger(row.age5_9), d.age10_14 = toInteger(row.age10_14),\n",
      "    d.age15_19 = toInteger(row.age15_19), d.age20_24 = toInteger(row.age20_24), \n",
      "    d.age25_34 = toInteger(row.age25_34), d.age35_44 = toInteger(row.age35_44), \n",
      "    d.age45_54 = toInteger(row.age45_54), d.age55_59 = toInteger(row.age55_59), \n",
      "    d.age60_64 = toInteger(row.age60_64), d.age65_74 = toInteger(row.age65_74), \n",
      "    d.age75_84 = toInteger(row.age75_84), d.age85_ = toInteger(row.age85_),\n",
      "    d.white = toInteger(row.white), d.blackOrAfricanAmerican = toInteger(row.blackOrAfricanAmerican), \n",
      "    d.americanIndianAndAlaskaNative = toInteger(row.americanIndianAndAlaskaNative), \n",
      "    d.asian = toInteger(row.asian), \n",
      "    d.nativeHawaiianAndOtherPacificIslander = toInteger(row.nativeHawaiianAndOtherPacificIslander),\n",
      "    d.otherRace = toInteger(row.otherRace), d.twoOrMoreRaces = toInteger(row.twoOrMoreRaces),\n",
      "    d.hispanicOrLatino = toInteger(row.hispanicOrLatino), \n",
      "    d.notHispanicOrLatino = toInteger(row.notHispanicOrLatino),\n",
      "    d.stateFips = row.stateFips, d.countyFips = row.countyFips,\n",
      "    d.source = row.source, d.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(d) as Demographics\n",
      ";\n",
      "// Special case for Puerto Rico: Counties become Admin1 divisions\n",
      "//USING PERIODIC COMMIT\n",
      "//LOAD CSV WITH HEADERS \n",
      "//FROM 'FILE:///03a-USCensusDP05Admin2.csv' AS row\n",
      "//WITH row WHERE row.stateFips = '72'\n",
      "//MATCH (a:Admin1{code: row.countyFips, country: 'PR'})\n",
      "//MATCH (d:Demographics{id: 'ACS5-' + row.stateFips + '-' + row.countyFips})\n",
      "//MERGE (a)-[h:HAS_DEMOGRAPHICS]->(d)\n",
      "//RETURN count(h) as HAS_DEMOGRAPHICS\n",
      "//;\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP05Admin2.csv' AS row\n",
      "MATCH (a:Admin2{fips: row.countyFips, stateFips: row.stateFips})\n",
      "MATCH (d:Demographics{id: 'ACSDP5Y2018.DP05-' + row.stateFips + '-' + row.countyFips})\n",
      "MERGE (a)-[h:HAS_DEMOGRAPHICS]->(d)\n",
      "RETURN count(h) as HAS_DEMOGRAPHICS\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP05Zip.csv' AS row \n",
      "MERGE (d:Demographics{id: 'ACSDP5Y2018.DP05-' + row.postalCode})\n",
      "SET d.name = 'Demographics-' + row.postalCode,\n",
      "    d.totalPopulation = toInteger(row.totalPopulation), \n",
      "    d.male = toInteger(row.male), d.female = toInteger(row.female),\n",
      "    d.age0_4 = toInteger(row.age0_4), d.age5_9 = toInteger(row.age5_9), d.age10_14 = toInteger(row.age10_14),\n",
      "    d.age15_19 = toInteger(row.age15_19), d.age20_24 = toInteger(row.age20_24), \n",
      "    d.age25_34 = toInteger(row.age25_34), d.age35_44 = toInteger(row.age35_44), \n",
      "    d.age45_54 = toInteger(row.age45_54), d.age55_59 = toInteger(row.age55_59), \n",
      "    d.age60_64 = toInteger(row.age60_64), d.age65_74 = toInteger(row.age65_74), \n",
      "    d.age75_84 = toInteger(row.age75_84), d.age85_ = toInteger(row.age85_),\n",
      "    d.white = toInteger(row.white), d.blackOrAfricanAmerican = toInteger(row.blackOrAfricanAmerican), \n",
      "    d.americanIndianAndAlaskaNative = toInteger(row.americanIndianAndAlaskaNative),\n",
      "    d.asian = toInteger(row.asian), \n",
      "    d.nativeHawaiianAndOtherPacificIslander = toInteger(row.nativeHawaiianAndOtherPacificIslander),\n",
      "    d.otherRace = toInteger(row.otherRace), d.twoOrMoreRaces = toInteger(row.twoOrMoreRaces),\n",
      "    d.hispanicOrLatino = toInteger(row.hispanicOrLatino), \n",
      "    d.notHispanicOrLatino = toInteger(row.notHispanicOrLatino),\n",
      "    d.postalCode = row.postalCode,\n",
      "    d.source = row.source, d.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(d) as Demographics\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP05Zip.csv' AS row\n",
      "MATCH (p:PostalCode{id: 'zip' + row.postalCode})\n",
      "MATCH (d:Demographics{id: 'ACSDP5Y2018.DP05-' + row.postalCode})\n",
      "MERGE (p)-[h:HAS_DEMOGRAPHICS]->(d)\n",
      "RETURN count(h) as HAS_DEMOGRAPHICS\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP05Tract.csv' AS row \n",
      "MERGE (d:Demographics{id: 'ACSDP5Y2018.DP05-' + row.tract})\n",
      "SET d.name = 'Demographics-' + row.tract, \n",
      "    d.totalPopulation = toInteger(row.totalPopulation), \n",
      "    d.male = toInteger(row.male), d.female = toInteger(row.female),\n",
      "    d.age0_4 = toInteger(row.age0_4), d.age5_9 = toInteger(row.age5_9), d.age10_14 = toInteger(row.age10_14),\n",
      "    d.age15_19 = toInteger(row.age15_19), d.age20_24 = toInteger(row.age20_24), \n",
      "    d.age25_34 = toInteger(row.age25_34), d.age35_44 = toInteger(row.age35_44), \n",
      "    d.age45_54 = toInteger(row.age45_54), d.age55_59 = toInteger(row.age55_59), \n",
      "    d.age60_64 = toInteger(row.age60_64), d.age65_74 = toInteger(row.age65_74), \n",
      "    d.age75_84 = toInteger(row.age75_84), d.age85_ = toInteger(row.age85_),\n",
      "    d.white = toInteger(row.white), \n",
      "    d.blackOrAfricanAmerican = toInteger(row.blackOrAfricanAmerican), \n",
      "    d.americanIndianAndAlaskaNative = toInteger(row.americanIndianAndAlaskaNative), \n",
      "    d.asian = toInteger(row.asian), \n",
      "    d.nativeHawaiianAndOtherPacificIslander = toInteger(row.nativeHawaiianAndOtherPacificIslander),\n",
      "    d.otherRace = toInteger(row.otherRace), d.twoOrMoreRaces = toInteger(row.twoOrMoreRaces),\n",
      "    d.hispanicOrLatino = toInteger(row.hispanicOrLatino), \n",
      "    d.notHispanicOrLatino = toInteger(row.notHispanicOrLatino),\n",
      "    d.tract = row.tract,\n",
      "    d.source = row.source, d.aggregationLevel = row.aggregationLevel\n",
      "RETURN count(d) as Demographics\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS \n",
      "FROM 'FILE:///03a-USCensusDP05Tract.csv' AS row\n",
      "MATCH (t:Tract{id: 'tract' + row.tract})\n",
      "MATCH (d:Demographics{id: 'ACSDP5Y2018.DP05-' + row.tract})\n",
      "MERGE (t)-[h:HAS_DEMOGRAPHICS]->(d)\n",
      "RETURN count(h) as HAS_DEMOGRAPHICS\n",
      ";Demographics\n",
      "3142\n",
      "HAS_DEMOGRAPHICS\n",
      "3142\n",
      "Demographics\n",
      "33120\n",
      "HAS_DEMOGRAPHICS\n",
      "33102\n",
      "Demographics\n",
      "73056\n",
      "HAS_DEMOGRAPHICS\n",
      "73056\n",
      " \n",
      "----------------------------------------------\n",
      "Running 10a-GeoLink.cypher:\n",
      " \n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLinkCruiseShip.csv' AS row\n",
      "MERGE (c:CruiseShip:Location{id: row.geoLocation})\n",
      "SET c.name = row.geoLocation, c.origLocation = row.origLocation\n",
      "RETURN count(c) as CruiseShip\n",
      ";\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLinkCruiseShip.csv' AS row\n",
      "MATCH (c:CruiseShip{id: row.geoLocation})\n",
      "MATCH (w:World:Location{id: \"m49:1\"})\n",
      "MERGE (c)-[i:IN]->(w)\n",
      "RETURN count(i) as CRUISESHIP_IN_WORLD\n",
      ";\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLinkCruiseShip.csv' AS row\n",
      "MATCH (c:CruiseShip{id: row.geoLocation})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "MERGE (s)-[h:FOUND_IN]->(c)\n",
      "RETURN count(h) as FOUND_IN_CRUISESHIP\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='0'\n",
      "MATCH (c:Country{name: row.geoName0})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(c)\n",
      "RETURN count(h) as FOUND_IN_0\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='1'\n",
      "MATCH (c:Country{name: row.geoName0})<-[:IN]-(a:Admin1{name: row.geoName1})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(a)\n",
      "RETURN count(h) as FOUND_IN_1_ADMIN1\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='1'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN*1..2]-(a:Admin2{name: row.geoName1})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(a)\n",
      "RETURN count(h) as FOUND_IN_1_ADMIN2\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='1'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN*1..3]-(c:City{name: row.geoName1})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(c)\n",
      "RETURN count(h) as FOUND_IN_1_CITY\n",
      ";\n",
      "// If no matches are found above, link by country only\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='1'\n",
      "MATCH (c:Country{name: row.geoName0})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(c)\n",
      "RETURN count(h) as FOUND_IN_1_COUNTRY\n",
      ";\n",
      "//\n",
      "// Location level 2\n",
      "//\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='2'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN]-(:Admin1{name: row.geoName1})<-[:IN]-(a:Admin2{name: row.geoName2})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(a)\n",
      "RETURN count(h) as FOUND_IN_2_ADMIN2\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='2'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN]-(:Admin1{name: row.geoName1})<-[:IN*1..2]-(c:City{name: row.geoName2})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(c)\n",
      "RETURN count(h) as FOUND_IN_2_ADMIN1_CITY\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='2'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN*1..2]-(:Admin2{name: row.geoName1})<-[:IN]-(c:City{name: row.geoName2})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(c)\n",
      "RETURN count(h) as FOUND_IN_2_ADMIN2_CITY\n",
      ";\n",
      "// If no matches are found, try linking by without geoName2\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='2'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN]-(a:Admin1{name: row.geoName1})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(a)\n",
      "RETURN count(h) as FOUND_IN_2_ADMIN1\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='2'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN*1..2]-(a:Admin2{name: row.geoName1})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(a)\n",
      "RETURN count(h) as FOUND_IN_2_ADMIN2\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='2'\n",
      "MATCH (c:Country{name: row.geoName0})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(c)\n",
      "RETURN count(h) as FOUND_IN_2_COUNTRY\n",
      ";\n",
      "//\n",
      "// Location level 3\n",
      "//\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='3'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN]-(:Admin1{name: row.geoName1})<-[:IN]-(:Admin2{name: row.geoName2})<-[:IN]-(c:City{name: row.geoName3})\n",
      "MATCH (s:Strain{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:FOUND_IN]->()\n",
      "MERGE (s)-[h:FOUND_IN]->(c)\n",
      "RETURN count(h) as FOUND_IN_3\n",
      ";\n",
      "//\n",
      "// Link Cases to Locations\n",
      "//\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLinkCruiseShip.csv' AS row\n",
      "MATCH (c:CruiseShip{id: row.geoLocation})\n",
      "MATCH (s:Cases{origLocation: row.origLocation})\n",
      "MERGE (s)-[h:REPORTED_IN]->(c)\n",
      "RETURN count(h) as REPORTED_IN_CRUISESHIP\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='0'\n",
      "MATCH (c:Country{name: row.geoName0})\n",
      "MATCH (s:Cases{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:REPORTED_IN]->()\n",
      "MERGE (s)-[h:REPORTED_IN]->(c)\n",
      "RETURN count(h) as REPORTED_IN_0\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='1'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN]-(a:Admin1{name: row.geoName1})\n",
      "MATCH (s:Cases{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:REPORTED_IN]->()\n",
      "MERGE (s)-[h:REPORTED_IN]->(a)\n",
      "RETURN count(h) as REPORTED_IN_1_ADMIN1\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='1'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN*1..2]-(a:Admin2{name: row.geoName1})\n",
      "MATCH (s:Cases{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:REPORTED_IN]->()\n",
      "MERGE (s)-[h:REPORTED_IN]->(a)\n",
      "RETURN count(h) as REPORTED_IN_1_ADMIN2\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='1'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN*1..3]-(c:City{name: row.geoName1})\n",
      "MATCH (s:Cases{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:REPORTED_IN]->()\n",
      "MERGE (s)-[h:REPORTED_IN]->(c)\n",
      "RETURN count(h) as REPORTED_IN_1_CITY\n",
      ";              \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='2'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN]-(:Admin1{name: row.geoName1})<-[:IN]-(a:Admin2{name: row.geoName2})\n",
      "MATCH (s:Cases{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:REPORTED_IN]->()\n",
      "MERGE (s)-[h:REPORTED_IN]->(a)\n",
      "RETURN count(h) as REPORTED_IN_2_ADMIN1_ADMIN2\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='2'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN]-(:Admin1{name: row.geoName1})<-[:IN*1..2]-(c:City{name: row.geoName2})\n",
      "MATCH (s:Cases{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:REPORTED_IN]->()\n",
      "MERGE (s)-[h:REPORTED_IN]->(c)\n",
      "RETURN count(h) as REPORTED_IN_ADMIN1_CITY\n",
      ";\n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='2'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN*1..2]-(:Admin2{name: row.geoName1})<-[:IN]-(c:City{name: row.geoName2})\n",
      "MATCH (s:Cases{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:REPORTED_IN]->()\n",
      "MERGE (s)-[h:REPORTED_IN]->(c)\n",
      "RETURN count(h) as REPORTED_IN_ADMIN2_CITY\n",
      ";                         \n",
      "USING PERIODIC COMMIT\n",
      "LOAD CSV WITH HEADERS\n",
      "FROM 'FILE:///10a-GeoLink.csv' AS row\n",
      "WITH row WHERE row.locationLevels='3'\n",
      "MATCH (:Country{name: row.geoName0})<-[:IN]-(:Admin1{name: row.geoName1})<-[:IN]-(:Admin2{name: row.geoName2})<-[:IN]-(c:City{name: row.geoName3})\n",
      "MATCH (s:Cases{origLocation: row.origLocation})\n",
      "WHERE NOT (s)-[:REPORTED_IN]->()\n",
      "MERGE (s)-[h:REPORTED_IN]->(c)\n",
      "RETURN count(h) as REPORTED_IN_3\n",
      ";\n",
      "\n",
      "                    \n",
      "\n",
      "CruiseShip\n",
      "7\n",
      "CRUISESHIP_IN_WORLD\n",
      "7\n",
      "FOUND_IN_CRUISESHIP\n",
      "42\n",
      "FOUND_IN_0\n",
      "8072\n",
      "FOUND_IN_1_ADMIN1\n",
      "35290\n",
      "FOUND_IN_1_ADMIN2\n",
      "295\n",
      "FOUND_IN_1_CITY\n",
      "2499\n",
      "FOUND_IN_1_COUNTRY\n",
      "289\n",
      "FOUND_IN_2_ADMIN2\n",
      "9738\n",
      "FOUND_IN_2_ADMIN1_CITY\n",
      "3642\n",
      "FOUND_IN_2_ADMIN2_CITY\n",
      "2\n",
      "FOUND_IN_2_ADMIN1\n",
      "256\n",
      "FOUND_IN_2_ADMIN2\n",
      "7\n",
      "FOUND_IN_2_COUNTRY\n",
      "805\n",
      "FOUND_IN_3\n",
      "819\n",
      "REPORTED_IN_CRUISESHIP\n",
      "707\n",
      "REPORTED_IN_0\n",
      "99396\n",
      "REPORTED_IN_1_ADMIN1\n",
      "160644\n",
      "REPORTED_IN_1_ADMIN2\n",
      "510068\n",
      "REPORTED_IN_1_CITY\n",
      "3454\n",
      "REPORTED_IN_2_ADMIN1_ADMIN2\n",
      "678300\n",
      "REPORTED_IN_ADMIN1_CITY\n",
      "8176\n",
      "REPORTED_IN_ADMIN2_CITY\n",
      "0\n",
      "REPORTED_IN_3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TODO create a batch script for Windows\n",
    "!../../scripts/run_cyphers.sh -init # start with a clean database and create new indices and constraints\n",
    "#!../../scripts/run_cyphers.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1196.8313286304474\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print('time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure to shutdown Neo4j!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping Neo4j........ stopped\n"
     ]
    }
   ],
   "source": [
    "!\"$NEO4J_HOME\"/bin/neo4j stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
